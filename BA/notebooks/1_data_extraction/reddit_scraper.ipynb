{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f4dd9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reddit Scraper for Bachelor Thesis Version 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5e37ce",
   "metadata": {
    "tags": [
     "Documentation"
    ]
   },
   "source": [
    "# Das ist die erste Version in der neuen Umgebung VS Code, da ich mit Jupyter Notebooks Probleme hatte. Ich hoffe, es wird besser funktionieren, da ich schon eine gewaltige Zeit verschwendet habe, die Probleme vergebens zu lösen. Hier wird wie in der ersten Version in Jupyter Notebooks ein Reddit Scraper programmiert. Ich werde Kommentare in Englisch lassen, da ich so die Codezeilen besser nachvollziehen kann. Ich werde zudem Updates in Deutsch unter dieser Zeile schreiben und nummerieren.\n",
    "\n",
    "#### Update 1: Initiales Setup und Umgebungskonfiguration\n",
    "# Ich habe mich dazu entschieden, meine virtuelle Python-Umgebung \"BA\" zu erstellen und VS Code als meine primäre Entwicklungsumgebung zu wählen. Bereits hier zeigte sich, dass die Probleme mit meiner Python-Installation, die ich zuvor in JupyterLab hatte, behoben werden konnten. Es stellte sich heraus, dass die generelle Python-Installation auf meinem Rechner fehlerhaft war, was wohl auch die Schwierigkeiten mit JupyterLab verursachte. Die Benutzeroberfläche von VS Code gefällt mir besser, und das Programm lädt generell schneller, weshalb ich den Rest meiner Arbeit hier abschließen werde. Um eine bessere Übersicht zu behalten, werde ich einmalige Installationen und Funktionen über das Terminal ausführen. Zudem habe ich black als Code-Formatter und isort für die Sortierung von Imports konfiguriert, um eine konsistente Code-Qualität sicherzustellen. Dies ist mir für die Lesbarkeit und Wartbarkeit des Projekts wichtig.\n",
    "\n",
    "#### Update 2: Projektstruktur und erste Schritte der Datenextraktion\n",
    "# Bislang habe ich das Gerüst für die Datenextraktion erstellt. Ich habe eine umfassende Projektverzeichnisstruktur definiert, die Ordner für Rohdaten, verarbeitete Daten, Notebooks, Quellcode, Berichte und Konfigurationsdateien umfasst. Dies soll die Organisation und Wartbarkeit des Projekts erheblich verbessern. Die Reddit-API-Kredentialien habe ich separat eingetragen, um sensible Informationen nicht direkt im Code zu speichern. Zunächst habe ich eine Handvoll Daten extrahiert, um die grundlegende Struktur und Funktionalität des Codes zu testen.\n",
    "\n",
    "#### Update 3: Sichere Kredentialverwaltung und Strukturimplementierung\n",
    "# Das Grundgerüst ist nun funktionsfähig, und ich habe erfolgreich Zugang zur Reddit API erhalten. Die Reddit API-Kredentialien sind nun in einer externen .env-Datei gespeichert, um den Best Practices der sicheren Datenverwaltung zu folgen. Um zu verhindern, dass diese sensiblen Informationen versehentlich veröffentlicht werden, habe ich die .env-Datei zusätzlich in die .gitignore-Datei aufgenommen. Die .gitignore-Datei weist Git an, welche Dateien und Ordner ignoriert werden sollen, sodass die .env-Datei nicht in das Repository hochgeladen wird. Für die Reddit API-Kredentialien habe ich eine eigene Anwendung in meinem Reddit-Account erstellt und die notwendigen Schlüssel dort entnommen. Die geplanten Hauptverzeichnisse des Stammverzeichnisses wurden ebenfalls angelegt.\n",
    "\n",
    "#### Update 4: Verfeinerung der Datenextraktionslogik\n",
    "# Nach weiterer Überlegung habe ich mich dazu entschieden, nur die Top-Kommentare zu extrahieren und Sub-Kommentare zu ignorieren. Obwohl Sub-Kommentare die Tiefe der Diskussion zeigen, sind sie stark vom Kontext des übergeordneten Kommentars abhängig und vor allem schwieriger zu vergleichen. Die Konzentration auf Top-Kommentare spart zudem Zeit bei der Abbildung komplexer Baumstrukturen und reicht für die Bewertung der Diskussionsqualität aus. Ich habe die post_limit und comment_limit angepasst, um etwa 800 Kommentare pro Turnier zu extrahieren, basierend auf meinen Schätzungen von 20-40 Posts pro Tag und 40-50 Top-Kommentaren pro Post. Redundanter Beispiel-Code für die allgemeine PRAW-API-Nutzung wurde entfernt, um den Code schlanker und fokussierter zu gestalten.\n",
    "\n",
    "#### Update 5: NLTK-Integration und erste Datenbereinigung\n",
    "# Für die Textanalyse habe ich die NLTK-Bibliothek integriert. Die notwendigen NLTK-Datenpakete (stopwords, punkt, wordnet, vader_lexicon) lade ich einmalig über das Terminal herunter, um das Notebook übersichtlich zu halten und unnötige Downloads bei jeder Ausführung zu vermeiden. Die ersten Schritte der Datenbereinigung habe ich bereits implementiert: Duplikate und leere Kommentare werden entfernt. Zudem filtere ich Kommentare von [deleted]-Autoren und solche heraus, deren Inhalt nach der Vorverarbeitung leer wäre, da sie für die Analyse nicht relevant sind.\n",
    "\n",
    "#### Update 6: Erweiterte Textverarbeitung und Sentiment-Analyse\n",
    "# Die Textverarbeitung habe ich nun vertieft. Ich habe eine Pipeline zur Bereinigung, Tokenisierung, Stop-Word-Entfernung und Lemmatisierung implementiert, um den Text optimal für die Analyse vorzubereiten. Dies beinhaltet das Entfernen von URLs, Sonderzeichen, Zahlen und die Umwandlung in Kleinbuchstaben, um den Text für die Analyse zu standardisieren. Anschließend wende ich die VADER-Sentiment-Analyse an, um die Polarität (positiv, neutral, negativ) und die Intensität des Sentiments in jedem Kommentar zu bestimmen. Zusätzlich berechne ich die Textlänge (Zeichen- und Wortanzahl) der Kommentare, da diese Metriken später für die Analyse des Engagements relevant sein könnten.\n",
    "\n",
    "#### Update 7: Zeitliche Kontextualisierung und Verhaltensanalyse\n",
    "# Um die Kommentare im zeitlichen Kontext der Turniere zu analysieren, habe ich Zeitperioden (Before Event, During Event, After Event) basierend auf den Turnierdaten markiert. Den zeitlichen Abstand jedes Kommentars zum Event-Start habe ich in Tagen berechnet, um die Dynamik besser zu verstehen. Erste Visualisierungen habe ich erstellt, um das Posting-Verhalten (Kommentare pro Stunde und Wochentag mittels Histogrammen und Heatmaps) und die Entwicklung der durchschnittlichen Kommentar-Scores im Zeitverlauf zu untersuchen. Diese ersten Einblicke sind entscheidend, um die Daten zu erfassen.\n",
    "\n",
    "#### Update 8: Umfassendes Feature Engineering und Modularisierung\n",
    "# Das Feature Engineering habe ich erheblich erweitert. Ich habe neue Features wie event_name, post_title_length, post_title_word_count, contains_question, contains_team_name, comment_to_post_score_ratio, author_karma (als Platzhalter) und comment_score_per_day hinzugefügt. Ein großer Schritt war die Modularisierung des Codes. Die Kernfunktionen für das Scraping (get_reddit_instance, get_posts_and_comments) habe ich in src/data/reddit_scraper.py ausgelagert. Alle textbezogenen Verarbeitungs- und Feature-Engineering-Funktionen (clean_text, preprocess_text, get_sentiment_score, calculate_text_length, contains_any_keyword) habe ich in src/features/text_features.py verschoben. Die Listen für Teamnamen und andere Keywords habe ich in separate JSON-Konfigurationsdateien (config/teams.json, config/keywords.json) ausgelagert, um die Wartbarkeit zu verbessern. Zudem habe ich eine Fehlerbehandlung in den PRAW-API-Aufrufen implementiert, um die Robustheit des Scrapers zu erhöhen. Die bereinigten und vorverarbeiteten Daten speichere ich nun effizient in einer SQLite-Datenbank, um eine schnelle Wiederverwendung zu ermöglichen.\n",
    "\n",
    "#### Update 9: Datenintegration und erste Analysen\n",
    "# Die bereinigten und mit Features angereicherten Daten speichere ich nun in einer SQLite-Datenbank (reddit_dota2_analysis.db) im data/processed/-Verzeichnis. Dafür habe ich Funktionen zur Datenbankverbindung, Tabellenerstellung (für Posts und Kommentare mit relationalen Beziehungen) und zum Einfügen der Daten implementiert. Nach der Speicherung lade ich die Daten direkt aus der Datenbank, um die weitere Analyse zu ermöglichen. Erste Pearson-Korrelationen zwischen den numerischen Features habe ich berechnet und visualisiert, um erste Einblicke in die Beziehungen zwischen den Variablen zu gewinnen. Anschließend habe ich die erste explorative Datenanalyse (EDA) durchgeführt. Die Daten beider Turniere (TI8 und TI11) habe ich in einem kombinierten DataFrame geladen, um vergleichende Analysen zu ermöglichen. Visualisierungen, um die Verteilung wichtiger Features wie Sentiment-Scores, Kommentar-Scores und Kommentarlängen zu untersuchen, habe ich erstellt. Zudem habe ich die Verteilung der Kommentare über die definierten Zeitperioden (Before Event, During Event, After Event) analysiert und den durchschnittlichen Sentiment-Score sowie den Kommentar-Score pro Zeitperiode für jedes Turnier visualisiert.\n",
    "\n",
    "#### Update 10: ML-Vorbereitung\n",
    "# Für die Machine Learning Vorbereitung habe ich Textdaten der Kommentare mittels TF-IDF-Vektorisierung in eine numerische Sparse Matrix umgewandelt. Parameter wie max_features, min_df, max_df und ngram_range habe ich konfiguriert, um die Dimensionalität zu steuern und relevante Features zu extrahieren. Zudem habe ich alle vorbereiteten Features (TF-IDF, numerische, kategoriale) zu einem einzigen Feature-Set zusammengeführt. Kategoriale Merkmale habe ich mittels One-Hot Encoding in ein numerisches Format umgewandelt. Die comment_score habe ich als Zielvariable für die Modellierung definiert. Die Daten habe ich in Trainings- und Testsets aufgeteilt, um die Modellentwicklung und eine unabhängige Modellbewertung zu ermöglichen. Die Daten sind nun vollständig für das Training von Machine Learning-Modellen vorbereitet. Außerdem habe ich die Code-Struktur neu organisiert, um die Modularität und Wartbarkeit zu verbessern.\n",
    "\n",
    "#### Update 11: Debugging und Optimierung des bestehenden Codes\n",
    "# Das Debugging hat eine erhebliche Zeit in Anspruch genommen. Die Reddit-API lieferte keine Kommentare für historische Daten, da meine Suchanfragen zu spezifisch waren und die Kommentare tief in den Posts versteckt lagen. Die Suchanfragen habe ich Stück für Stück optimiert und subreddit.search() und subreddit.top() kombiniert, um mehr Posts zu finden. Das Abrufen der Kommentare war ineffizient, da replace_more(limit=None) zu viele API-Aufrufe verursachte. Diese habe ich auf limit=3 angepasst, um einen Kompromiss zwischen Datenmenge und Geschwindigkeit zu finden. Um die Effizienz weiter zu steigern, habe ich die Anzahl der Posts, für die Kommentare verarbeitet werden (max_posts_to_process), auf 15 reduziert. Timeouts und API-Fehler waren ebenfalls ein Problem. Hierfür habe ich die tenacity-Bibliothek mit Retry-Mechanismen und Exponential Backoff implementiert, um meine API-Aufrufe robuster zu machen. Diverse KeyError, LookupError und SyntaxError durch fehlende NLTK-Daten, falsche Spaltennamen oder Modularisierungsfehler habe ich ebenfalls behoben.\n",
    "\n",
    "#### Update 12: Fertigstellung der Datenextraktion und -vorbereitung\n",
    "# Die Datenextraktion und -vorbereitung sind nun abgeschlossen. Die Pipeline ist stabil und effizient, die Daten sind gesammelt, bereinigt, mit Features angereichert und in SQLite gespeichert. Die Modularisierung des Codes verbessert die Wartbarkeit erheblich. Ich werde in Zukunft schauen, ob sich meine Worte bewahrheiten, wenn ich einen Spieler genauer unter die Lupe nehmen werde. Vorerst fahre ich aber mit den Turnieren fort und behandle den Spieler als separate Instanz. Jetzt beginne ich mit der Analyse und Modellierung.\n",
    "\n",
    "#### Update 13: Analyse der Turniere TI8 und TI11\n",
    "# Nachdem die Daten erfolgreich geladen und konsolidiert wurden, begann für mich die Phase der Explorativen Datenanalyse (EDA) und Visualisierung. Mein Ziel war es, tiefere Einblicke in die Dynamik des Online-Engagements während der Turniere TI8 und TI11 zu gewinnen. Zunächst habe ich die Verteilung des Compound Sentiment Scores für beide Events untersucht. Hierbei zeigte sich eine bimodale Verteilung mit dominanten Peaks bei neutralem (0.00) und stark positivem (0.75-1.00) Sentiment. Negative Sentiments waren deutlich seltener. Ein Vergleich zwischen TI8 und TI11 offenbarte, dass TI11 tendenziell eine etwas höhere Konzentration an neutralen und stark positiven Kommentaren aufwies, was auf eine möglicherweise sachlichere oder insgesamt positivere Diskussionskultur hindeutet.\n",
    "# Anschließend habe ich die Verteilung des Kommentar-Scores (comment_score) analysiert. Beide Verteilungen waren extrem rechtsschief, was bedeutet, dass die überwiegende Mehrheit der Kommentare einen sehr niedrigen Score (nahe 0) hatte, während nur wenige Kommentare hohe Scores erreichten – ein typisches \"Long Tail\"-Phänomen in sozialen Medien. Interessanterweise zeigte TI11 tendenziell mehr Kommentare mit moderat höheren Scores als TI8, obwohl OGs Sieg bei TI8 als emotionaler wahrgenommen wurde. Dies deutet darauf hin, dass ein hoher Score nicht zwingend mit positivem Sentiment korreliert und andere Faktoren wie Informationsgehalt oder Humor eine Rolle spielen könnten. Die Analyse der Wortanzahl (word_count) der Kommentare bestätigte ebenfalls eine extrem rechtsschiefe Verteilung, wobei die meisten Kommentare sehr kurz waren (Modus bei etwa 10 Wörtern). TI11 wies hier eine höhere Frequenz von kürzeren Kommentaren auf, was auf eine breitere, aber prägnantere Beteiligung hindeuten könnte. Die geringe Korrelation zwischen Wortanzahl und Kommentar-Score unterstreicht, dass die Länge allein kein primärer Treiber für die Popularität eines Kommentars ist.\n",
    "# Ein weiterer Fokus lag auf der zeitlichen Verteilung der Kommentare. Die Analyse zeigte eine extreme Konzentration des Engagements während der Events (\"During Event\"), mit einer überwältigenden Mehrheit der Kommentare in dieser Periode. Die Phasen \"Before Event\" und \"Outside Window\" wiesen keine Kommentare auf, und die Aktivität fiel nach dem Event (\"After Event\") rapide ab. Dieses Muster war für beide Turniere konsistent und unterstreicht die Echtzeit- und reaktive Natur der Diskussionen auf Reddit während großer E-Sports-Events. Die durchschnittlichen Kommentar-Scores nach Zeitperiode zeigten, dass TI8 während des Events höhere, aber variablere Scores hatte, während TI11 homogenere, aber niedrigere Scores aufwies. Nach dem Event schien das Engagement für TI8 etwas nachhaltiger zu sein, während es für TI11 fast auf Null fiel. Diese detaillierten Analysen lieferten wertvolle Erkenntnisse über die unterschiedliche Dynamik des Engagements und der Diskussionskultur zwischen den beiden Turnieren.\n",
    "\n",
    "#### Update 14: Erste Modellierungsversuche und kritische Fehlerbehebung\n",
    "# Nach der umfassenden Explorativen Datenanalyse (Update 13) war der nächste logische Schritt die Anwendung von Machine Learning-Modellen. Mein Ziel war es, das Engagement der Kommentare (comment_score) vorherzusagen und erste Hypothesen über die treibenden Faktoren zu überprüfen.\n",
    "# Ich begann mit der Implementierung eines Multiple Linearen Regressionsmodells als unsere Baseline. Dieses Modell sollte die linearen Beziehungen zwischen unseren aufbereiteten Features und der Zielvariable aufzeigen. Parallel dazu habe ich einen XGBoost Regressor trainiert, ein leistungsstärkeres Ensemble-Modell, das auch komplexe, nicht-lineare Muster in den Daten erkennen kann. Beide Modelle habe ich auf einem separaten Testset evaluiert, um ihre Vorhersagekraft zu beurteilen.\n",
    "# Ein kritischer Moment in dieser Phase war die Beobachtung einer unrealistisch perfekten Modellleistung (R²-Werte von 1.00 und MSE-Werte nahe 0.00). Eine sofortige Analyse der Feature Importance des XGBoost-Modells enthüllte die Ursache: Die Zielvariable (comment_score) war unbeabsichtigt selbst als Feature in den Trainingsdaten enthalten. Dies führte zu einem klassischen Datenleck (Data Leakage), bei dem das Modell die Antwort bereits \"kannte\". Dieser Fehler wurde umgehend behoben, indem die Zielvariable konsequent aus dem Feature-Set entfernt wurde.\n",
    "# Nach dieser entscheidenden Korrektur zeigten die Modelle realistische Leistungswerte, die eine fundierte Analyse ermöglichten. Die Feature Importance des XGBoost-Modells lieferte nun aussagekräftige Einblicke, welche Faktoren den comment_score tatsächlich beeinflussten. Die trainierten Modelle habe ich anschließend persistent gespeichert, um die Reproduzierbarkeit zu gewährleisten und eine spätere Nutzung zu ermöglichen.\n",
    "\n",
    "#### Update 15: Modularisierung der Modellierung und Pipeline-Integration\n",
    "# Um die Robustheit, Wartbarkeit und Skalierbarkeit meiner Machine Learning-Pipeline zu verbessern, habe ich eine umfassende Modularisierung des gesamten Modellierungsprozesses vorgenommen. Die zuvor im Notebook enthaltenen Schritte für die ML-Vorbereitung, das Training, Tuning und die Evaluierung habe ich in dedizierte Python-Skripte ausgelagert.\n",
    "# Der Kern dieser Neuausrichtung ist die Einführung von Scikit-learn Pipelines. Die gesamte Datenvorverarbeitung – von der TF-IDF-Vektorisierung der Texte über das One-Hot-Encoding kategorialer Merkmale bis hin zum Feature Scaling numerischer Variablen – habe ich in einer ColumnTransformer-basierten Pipeline gekapselt. Diese Pipeline stellt sicher, dass alle Transformationen konsistent und ohne Datenlecks angewendet werden, indem sie nur auf den Trainingsdaten gelernt und dann auf alle weiteren Daten (Testset, Cross-Validation) angewendet wird.\n",
    "# Zusätzlich habe ich fortgeschrittene Validierungs- und Optimierungstechniken integriert: Cross-Validation wurde für beide Modelle (Lineare Regression und XGBoost) implementiert, um eine robustere Schätzung der Modellleistung zu ermöglichen. Für das leistungsstarke XGBoost-Modell habe ich eine RandomizedSearchCV implementiert, um effizient den Hyperparameter-Raum zu durchsuchen und die optimale Konfiguration zu finden. Um die \"Black-Box\" des XGBoost-Modells zu öffnen und die Beiträge einzelner Features besser zu verstehen, habe ich SHAP (SHapley Additive exPlanations)-Werte berechnet, die aussagekräftige Plots zur globalen Feature Importance und zu den Auswirkungen einzelner Features generieren.\n",
    "# Der gesamte Workflow wird nun durch ein zentrales Skript (train_model.py) orchestriert, das die modularen Funktionen aus model_utils.py aufruft. Dies schafft eine saubere Trennung der Verantwortlichkeiten und ermöglicht eine effiziente und reproduzierbare Ausführung der gesamten Machine Learning-Pipeline.\n",
    "\n",
    "#### Update 16: Erfolgreiche Forschung und zukünftige Erweiterung des Analysefokus\n",
    "# Die bisherige Forschungsarbeit war äußerst erfolgreich und hat mir ein tiefes Verständnis für die Dynamik des Online-Engagements auf Reddit im Kontext von E-Sports-Turnieren (TI8 und TI11) ermöglicht. Ich habe eine robuste Datenpipeline etabliert, aussagekräftige Features entwickelt und leistungsstarke Machine Learning-Modelle trainiert, die mir erste Einblicke in die Einflussfaktoren des Kommentar-Engagements geben.\n",
    "# Angesichts dieser soliden Grundlage und der modularen Architektur meines Projekts ist es nun an der Zeit, meinen Analysefokus zu erweitern. Um die Komplexität und Nuancen des Online-Engagements noch umfassender zu beleuchten, werde ich zwei neue Variablen in meine Untersuchung integrieren: Erstens, die Spieler-Personality und Medienpräsenz, mit einem Fokus auf Topson. Hierbei werde ich analysieren, wie die persönliche Aktivität und Medienpräsenz eines ikonischen Spielers wie Topson das Engagement auf Reddit beeinflusst, insbesondere im Kontext seines Comebacks und der damit verbundenen Nostalgie und Hype. Zweitens, Organisationen im Umbruch, mit einem Fokus auf OG in 2024/2025. Diese Variable ermöglicht es mir, die Fanreaktionen und die Diskussionsqualität zu untersuchen, wenn ein ehemals dominantes Team wie OG eine Phase des Umbruchs oder des Misserfolgs durchläuft. Dies bietet einen spannenden Kontrast zu den Erfolgsphasen, die ich bereits analysiert habe.\n",
    "# Diese Erweiterung wird es mir ermöglichen, einen analytischen Bogen von \"Erfolg\" über \"Dominanz\" bis hin zu \"Neuorientierung\" zu spannen und die vielfältigen Ausdrucksformen des Fan-Engagements bei Turnieren, Teams und individuellen Spielern zu erforschen. Die Integration dieser neuen Variablen sollte dank unserer modularen Codebasis mit minimalem Aufwand möglich sein, da die bestehenden Datenextraktions-, Vorverarbeitungs- und Modellierungsschritte nahtlos wiederverwendet werden können. Es ist ein absichtlicher Test, um zu sehen, ob meine Codestruktur tatsächlich gut genug ist.\n",
    "\n",
    "#### Update 17: Erweiterung des Datensatzes - Integration von Riyadh Masters 2024\n",
    "# Nachdem die modulare Pipeline für die Datenextraktion und -verarbeitung stabilisiert war, begann die Erweiterung des Datensatzes um die neuen Analysevariablen. Der Fokus lag auf dem Riyadh Masters 2024, einem Turnier, das sowohl die Präsenz von OG (mit schwacher Leistung) als auch von Topson (als Stand-In mit guter Performance) umfasste.\n",
    "# Zunächst erfolgte eine umfassende Aktualisierung der keywords.json-Datei. Diese wurde um alle relevanten Spieler- und Coach-Namen der am Riyadh Masters 2024 teilnehmenden Teams erweitert. Zudem wurden turnierspezifische Begriffe wie \"riyadh masters\", \"gamers8\" und \"esports world cup\" sowie Platzierungen wie \"ninth place\" bis \"twelveth place\" in die tournament_event_keywords aufgenommen. Die Listen wurden alphabetisch sortiert und Duplikate entfernt, um die Qualität der Feature-Erkennung zu optimieren.\n",
    "# Anschließend wurden im reddit_scraper.ipynb-Notebook neue Abschnitte zur Datensammlung für OG und Topson während des Riyadh Masters 2024 hinzugefügt. Hierfür wurden die bereits etablierten Funktionen zur Datenextraktion (get_posts_and_comments) mit spezifischen Suchanfragen (\"OG\", \"Topson\") und dem relevanten Turnierzeitraum (04. bis 28. Juli 2024) verwendet.\n",
    "# Die neu gesammelten Daten für OG und Topson beim Riyadh Masters 2024 wurden nahtlos in die bestehende Datenbereinigungs- und Feature-Engineering-Pipeline integriert. Alle zuvor implementierten Schritte, wie die initiale Bereinigung, Textvorverarbeitung, Sentiment-Analyse, Zeitperioden-Markierung und die Erstellung weiterer Features (z.B. post_title_features, contains_question, contains_team_name, comment_to_post_score_ratio), wurden konsequent auf die neuen DataFrames angewendet. Jedem neuen Datensatz wurde ein eindeutiger event_name (OG_RM24, Topson_RM24) zugewiesen, um eine differenzierte Analyse zu ermöglichen.\n",
    "# Abschließend wurden alle bereinigten und mit Features angereicherten DataFrames (TI8, TI11, OG_RM24, Topson_RM24) zu einem einzigen großen DataFrame zusammengeführt und in der SQLite-Datenbank gespeichert. Diese Konsolidierung stellt sicher, dass die nachfolgenden Schritte der Explorativen Datenanalyse (EDA) und der Machine Learning-Modellierung automatisch alle Events umfassen und die Vorteile der modularen Projektstruktur voll genutzt werden können.\n",
    "\n",
    "#### Update 18: Finale Phase der Modularisierung und Codeoptimierung\n",
    "# Die letzten Wochen waren eine intensive und zugleich sehr aufschlussreiche Phase der Projektentwicklung. Mein Hauptaugenmerk lag darauf, die Modularität und Wartbarkeit des gesamten Codes auf ein Niveau zu heben, das den hohen Ansprüchen meiner Bachelorarbeit gerecht wird. Es war ein iterativer Prozess, der sich durch viele kleine, aber entscheidende Schritte zog, um die Struktur zu perfektionieren.\n",
    "# Ein großer Schritt war die vollständige Auslagerung der Datenextraktion, -vorbereitung und des Feature Engineerings in die prepare_data.py-Datei. Diese Datei war nicht mein erster Versuch, das Notebook zu entlasten, sondern tatsächlich das allerletzte Modul, das ich in diesem Kontext erstellt habe. Es repräsentiert die Kulmination meiner Modularisierungsbemühungen und ist nun eine robuste, eigenständige Pipeline. Ich habe mich dazu entschieden, die gesamte Reddit API-Initialisierung, die zuvor noch im Notebook stattfand, direkt in prepare_data.py zu integrieren. Das bedeutet, die Funktion prüft nun selbst, ob Daten in der SQLite-Datenbank (reddit_dota2_analysis.db) vorhanden sind, und sammelt sie nur bei Bedarf neu. Das spart nicht nur enorme Zeit bei jedem Durchlauf, sondern schont auch die API-Limits – ein Problem, das mich in Update 11 schon viel Zeit gekostet hat.\n",
    "# Die zentrale Konfiguration in config.py hat sich als absolut unverzichtbar erwiesen. Alle relevanten Parameter, von den Turnierdaten über die API-Limits bis hin zu den Pfaden und Modell-Hyperparametern, sind nun dort gebündelt. Es stellte sich heraus, dass dies der Schlüssel zu einer konsistenten und leicht anpassbaren Codebasis ist, was für die Reproduzierbarkeit meiner Ergebnisse entscheidend ist.\n",
    "# Auch die Visualisierungslogik habe ich weiter optimiert. Ich habe eine neue Orchestrierungsfunktion, generate_all_eda_plots, in plots.py implementiert, die nun alle explorativen Datenanalyse-Plots mit einem einzigen Aufruf generiert. Sogar die Pearson-Korrelationsanalyse, die bisher noch direkt im Notebook stattfand, wurde konsequent in eine eigene Funktion (plot_pearson_correlation) in plots.py ausgelagert. Rückblickend muss ich zugeben, dass die Pearson-Korrelation noch hardcodiert war, weil ich sie schlichtweg übersehen und vergessen hatte – ein gutes Beispiel dafür, wie wichtig eine systematische Überprüfung ist. Das macht die Visualisierungen nicht nur konsistenter, sondern auch viel einfacher zu handhaben.\n",
    "# Parallel dazu habe ich alle unterstützenden Module – database_utils.py, preprocess.py, reddit_scraper.py, feature_engineering.py, text_features.py und config_loader.py – einer umfassenden Überprüfung unterzogen. Dabei wurden kleinere Inkonsistenzen behoben, Logging-Konfigurationen vereinheitlicht und redundante Code-Abschnitte entfernt. Besonders die Anpassung von database_utils.py, die nun nur noch die relevanten Kommentardaten zurückgibt, trägt zur Präzision bei.\n",
    "# Das reddit_scraper.ipynb-Notebook selbst ist nun das, was es sein sollte: ein klarer, übersichtlicher Bericht und Orchestrator meines gesamten Workflows. Redundante Imports wurden entfernt, die Nummerierung der Abschnitte wurde überarbeitet, um eine durchgängige und logische Reihenfolge zu gewährleisten, und die Hinweise zur Datenerfassung wurden für verschiedene Betriebssysteme präzisiert.\n",
    "# Mit diesen umfassenden Anpassungen ist die Kern-Programmierarbeit nun abgeschlossen. Ich habe eine robuste, effiziente und vollständig modularisierte Pipeline geschaffen, die mir die Gewissheit gibt, mich in den kommenden Wochen voll und ganz auf die Analyse der Ergebnisse und das Verfassen meiner Bachelorarbeit konzentrieren zu können. Es ist ein gutes Gefühl, diese technische Grundlage geschaffen zu haben.\n",
    "# Es folgt lediglich nur noch das Debugging und die Analyse von Riyadh Masters 2024. Danach sollte ich ich komplett fertig mit dem Programmier-Aspekt werden.\n",
    "\n",
    "#### Update 19: Debugging und Analyse von Riyadh Masters 2024\n",
    "# Die Zeit seit dem letzten Update war eine Phase intensiver Verfeinerung und des Debuggings, die darauf abzielte, die analytische Tiefe und die Robustheit meiner gesamten Codebasis zu maximieren.\n",
    "# Ein zentraler Schritt war die Optimierung meiner Analysemethoden. Ich habe die Visualisierungsfunktionen in plots.py erheblich erweitert, um nicht nur ansprechende Diagramme zu generieren, sondern diese auch direkt mit quantitativen Metriken zu versehen. Das bedeutet, meine Plots zeigen nun nicht mehr nur visuelle Trends, sondern untermauern diese mit konkreten Mittelwerten, Medianen und Standardabweichungen. Dies ist entscheidend, um \"Schätzungen nach Augenmaß\" zu vermeiden und meine Beobachtungen wissenschaftlich präzise zu belegen. Zudem habe ich die Qualität der Plot-Ausgabe für meine Bachelorarbeit optimiert, indem alle Diagramme nun automatisch in hoher Auflösung und in einem konsistenten Format gespeichert werden. Die generierten Bilder finden sich nun im Ordner Code/BA/reports/figures/ und die detaillierten Logs im Ordner Code/BA/logs/.\n",
    "# Eine Implementierung für statistische Tests war für die Analyse nötig. Ich habe ein neues Modul (statistical_tests.py) erstellt, das Funktionen für unabhängige T-Tests, ANOVA und Chi-Quadrat-Tests enthält. Meine bisherigen visuellen Analysen und \"Schätzungen nach Augenmaß\" reichten nicht aus, um eine vollständige und wissenschaftlich fundierte Antwort auf meine Forschungsfragen zu geben. Diese Tests sind nun integriert, um die beobachteten Unterschiede quantitativ zu untermauern und die Ergebnisse besser zu verstehen und zu vergleichen.\n",
    "# Ich habe die text_features.py um einen automatischen NLTK-Daten-Download-Check erweitert. Dies stellt sicher, dass alle notwendigen Sprachressourcen vorhanden sind, bevor die Textverarbeitung beginnt, und macht das Modul widerstandsfähiger gegen fehlende Abhängigkeiten.\n",
    "# Ein erheblicher Teil meiner Zeit floss in das finale Debugging und die Konsistenz des Loggings. Es stellte sich heraus, dass die Art und Weise, wie das Logging in den verschiedenen Modulen konfiguriert war, zu unerwartetem Verhalten führte. Ich habe nun eine umfassende Lösung implementiert, bei der logging.basicConfig() nur noch an den zentralen Einstiegspunkten meiner Anwendung (dem Notebook und den direkt ausführbaren Skripten) aufgerufen wird. Alle anderen Module verwenden nun eine logger-Instanz, was eine durchgängige und zuverlässige Protokollierung des gesamten Workflows gewährleistet. Dies war ein hartnäckiges Problem, das viel Geduld erforderte, aber nun ist der Output meiner Skripte klar und vollständig.\n",
    "# Die Zusammenfassung der Machine Learning Ergebnisse wurde ebenfalls hinzugefügt. Ich muss zugeben, dass diese Funktion anfangs übersehen wurde und ich ihre Notwendigkeit erst jetzt bemerkt habe. Sie speichert die wichtigsten Performance-Metriken und Hyperparameter in einer separaten Textdatei, was für die Dokumentation meiner Arbeit sehr nützlich ist. Gleiches gilt für die Speicherung der Rohdaten: Auch diese wurde erst jetzt implementiert, um die Reproduzierbarkeit meiner Datenbasis zu gewährleisten.\n",
    "# Schließlich habe ich die Projektstruktur noch einmal kritisch beleuchtet und bestätigt, dass die aktuelle Organisation meiner Ordner (BA/config, BA/data/raw, BA/data/processed, BA/models, BA/reports, BA/logs etc.) den Best Practices entspricht und eine klare Trennung der Verantwortlichkeiten gewährleistet. Die finale Runde zum Überschauen aller Module und Funktionen ist damit bereits passiert.\n",
    "# Ich denke, dass die Codebasis für meine Bachelorarbeit genügend ist und ich hiermit hoffentlich fertig bin zu codieren. Das nächste Update wird sich ausschließlich der letzten Feinschliffe der Kommentare und Markdowns widmen und damit der letzte Teil des Programmierabschnitts sein.\n",
    "\n",
    "# Update 20: Finale Feinschliffe, Logging-Optimierung und Abschluss des Programmierteils\n",
    "# Nach den intensiven Arbeiten der vergangenen Updates stand nun die endgültige Feinarbeit an, um die Codebasis auf einen konsistenten und reproduzierbaren Endstand zu bringen. Ein Schwerpunkt lag auf der Vereinheitlichung und klaren Trennung der Logging-Struktur. Es werden nun drei separate Log-Dateien geführt – für Datenextraktion und -verarbeitung, EDA inkl. statistischer Tests sowie Machine Learning. Diese Trennung erhöht die Übersichtlichkeit und erleichtert sowohl das Debugging als auch die Nachvollziehbarkeit einzelner Pipeline-Schritte.\n",
    "# Im Zuge dieser Arbeiten habe ich sämtliche Debugging-Reste aus dem Code entfernt und die Kommentare sowie Markdown-Zellen im Notebook überarbeitet, um eine klare und einheitliche Dokumentation sicherzustellen. Ein größerer Fehler trat im ML-Bereich auf: Die Generierung der ML-Plots brach ab, bevor die Schleife starten konnte. Ursache war eine fehlerhafte Verarbeitung der all_feature_names-Liste in der Funktion interpret_model_shap. Die Liste wurde nicht als numpy.ndarray übergeben, wodurch eine if-Bedingung fehlschlug. Nach Anpassung der entsprechenden Bedingung konnten alle ML-Plots erfolgreich erstellt werden. \n",
    "# Mit diesen Anpassungen ist der Programmierteil nun abgeschlossen. Die Codebasis erfüllt die Ziele Modularität, Reproduzierbarkeit und vollständige Dokumentation. Als letzten Schritt plane ich, das gesamte Projekt auf einem anderen Rechner bzw. Laptop auszuführen, um die Reproduzierbarkeit unabhängig von der aktuellen Entwicklungsumgebung zu bestätigen. Damit ist die technische Grundlage für die Bachelorarbeit finalisiert.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0141200",
   "metadata": {
    "tags": [
     "Guide on How to use the project"
    ]
   },
   "source": [
    "# ### **Comprehensive Project Reproduction Guide in VS Code**\n",
    "#\n",
    "# This section provides a detailed, step-by-step guide to fully reproduce this project. It is highly recommended to use Visual Studio Code (VS Code) as your development environment, as it was specifically designed for seamless integration with Python and Jupyter Notebooks.\n",
    "#\n",
    "# #### **1. Prerequisites & Initial Setup**\n",
    "#\n",
    "# Before you begin, ensure the following prerequisites are met:\n",
    "#\n",
    "# *   **Visual Studio Code:** Installed on your system.\n",
    "# *   **Python 3.x:** A recent version of Python is installed.\n",
    "#\n",
    "# **Steps:**\n",
    "#\n",
    "# 1.  **Open the Project Folder:**\n",
    "#     *   Download or clone the entire project repository.\n",
    "#     *   Open VS Code.\n",
    "#     *   Go to `File` > `Open Folder...` and select the main project directory (e.g., `C:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code`).\n",
    "#\n",
    "# 2.  **Install Essential VS Code Extensions:**\n",
    "#     These extensions are crucial for working seamlessly with Python code and Jupyter Notebooks in VS Code.\n",
    "#     *   Open the Extensions view in VS Code (Ctrl+Shift+X or Cmd+Shift+X).\n",
    "#     *   Search for and install the following extensions by Microsoft:\n",
    "#         *   **Python:** Provides IntelliSense, debugging, code formatting, and more.\n",
    "#         *   **Jupyter:** Enables opening, editing, and running `.ipynb` files directly within VS Code.\n",
    "#     *   **Note:** These two extensions are fully sufficient for reproducing the project's results. Other extensions are generally for development convenience but are not strictly necessary for this purpose.\n",
    "#\n",
    "# #### **2. Set Up the Virtual Environment**\n",
    "#\n",
    "# A virtual environment isolates project dependencies from other Python projects on your system, ensuring consistent results.\n",
    "#\n",
    "# 1.  **Open the Integrated Terminal:**\n",
    "#     *   In VS Code, go to `Terminal` > `New Terminal`. This will open a terminal session (typically PowerShell on Windows) directly within your project's root directory.\n",
    "#\n",
    "# 2.  **Adjust PowerShell Execution Policy (Windows Only):**\n",
    "#     *   If you are on Windows and encounter an error like \"cannot be loaded because running scripts is disabled on this system\" when activating the virtual environment, you need to temporarily bypass the execution policy.\n",
    "#     *   In the **VS Code Terminal**, run the following command. Confirm the change by typing `J` (Yes) or `A` (All) and pressing `Enter`. This change is valid only for the current terminal session.\n",
    "#         ```powershell\n",
    "#         Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass\n",
    "#         ```\n",
    "#\n",
    "# 3.  **Create a Virtual Environment:**\n",
    "#     *   In the **VS Code Terminal**, run:\n",
    "#         ```bash\n",
    "#         python -m venv venv\n",
    "#         ```\n",
    "#     *   **Expected Output:** There should be no error messages. A new folder named `venv` will be created in your project directory.\n",
    "#\n",
    "# 4.  **Activate the Virtual Environment:**\n",
    "#     *   In the **VS Code Terminal**, run:\n",
    "#         ```bash\n",
    "#         .\\venv\\Scripts\\activate\n",
    "#         ```\n",
    "#     *   **Expected Output:** Your terminal prompt should change to include `(venv)` at the beginning (e.g., `(venv) PS C:\\Your\\Project\\Path>`). This confirms the virtual environment is active.\n",
    "#\n",
    "# 5.  **Select the Python Interpreter in VS Code:**\n",
    "#     *   This is a **critical step** for VS Code to use the Python version from your virtual environment.\n",
    "#     *   In the **bottom-left corner** OR **upper-right corner** of the VS Code window, click on the displayed Python version (e.g., `Python 3.x.x`).\n",
    "#     *   A list of interpreters will appear at the top. Select the interpreter that points to your newly created virtual environment (it will typically have `('venv')` next to it, e.g., `Python 3.x.x ('venv')`).\n",
    "#     *   **Expected Output:** The Python version in the bottom bar should now display `Python 3.x.x (venv)`.\n",
    "#\n",
    "# 6.  **Install Dependencies:**\n",
    "#     *   Ensure the `requirements.txt` file is present in your project's root directory.\n",
    "#     *   In the **VS Code Terminal** (with `(venv)` active), run:\n",
    "#         ```bash\n",
    "#         pip install -r requirements.txt\n",
    "#         ```\n",
    "#     *   **Expected Output:** A series of messages indicating the installation of required libraries. It should conclude with `Successfully installed ...`.\n",
    "#\n",
    "# #### **3. Execute the Project Workflow**\n",
    "#\n",
    "# The project workflow is structured into three main steps: Data Preparation, Exploratory Data Analysis (EDA) & Statistical Tests, and Machine Learning.\n",
    "#\n",
    "# ##### **3.1 Step 1: Data Preparation (`prepare_data.py`)**\n",
    "#\n",
    "# This script is the initial step. It collects raw data from the Reddit API, performs cleaning and preprocessing, extracts features, and saves the processed data into an SQLite database.\n",
    "# This step will be automatically executed when running the subsequent EDA & Statistical Tests notebook. However, you can also run it independently.\n",
    "# If you wish to manually execute this step, follow these instructions:\n",
    "#\n",
    "# 1.  **Open Terminal & Activate Environment:**\n",
    "#     *   Ensure you have a **VS Code Terminal** open and your virtual environment (`(venv)`) is active.\n",
    "#\n",
    "# 2.  **Ensure Unicode Output for Terminal (Optional, but Recommended):**\n",
    "#     *   To prevent `UnicodeEncodeError` when special characters (like emojis in log messages) are printed to the terminal, set the `PYTHONIOENCODING` environment variable to `utf-8`. This needs to be done in each new terminal session where you intend to run Python scripts.\n",
    "#         ```powershell\n",
    "#         $env:PYTHONIOENCODING=\"utf-8\"\n",
    "#         ```\n",
    "#     *   **Clarification:** This instruction is specifically for the terminal output of `.py` scripts. Jupyter Notebooks handle their output differently, so this step is not strictly necessary for notebook cells themselves, but crucial for direct script execution in the terminal.\n",
    "#\n",
    "# 3.  **Execute Data Preparation Script:**\n",
    "#     *   In the **VS Code Terminal**, run:\n",
    "#         ```bash\n",
    "#         python \"BA\\src\\data\\prepare_data.py\"\n",
    "#         ```\n",
    "#     *   **Expected Outputs in Terminal:**\n",
    "#         *   Detailed log messages about the progress of data extraction, cleaning, and feature engineering.\n",
    "#         *   Messages like \"Successfully collected X comments for event_name.\"\n",
    "#         *   Confirmation of database saving: \"Data storage in SQLite complete.\"\n",
    "#         *   Summary of prepared data: \"Data preparation successful. Prepared DataFrame shape: (X, Y)\".\n",
    "#         *   **Important Note:** If you make changes to the data processing logic and wish to re-collect data, you **MUST** manually delete the existing SQLite database file (`reddit_dota2_analysis.db` located in the `BA\\data\\processed` folder) before running this script again.\n",
    "#\n",
    "# ##### **3.2 Step 2: Exploratory Data Analysis (EDA) & Statistical Tests (Jupyter Notebook)**\n",
    "#\n",
    "# This Jupyter Notebook (`reddit_scraper.ipynb`) loads the prepared data, performs exploratory data analysis, generates visualizations, and conducts statistical tests.\n",
    "#\n",
    "# 1.  **Open the Notebook:**\n",
    "#     *   Navigate in the VS Code Explorer to the desired `.ipynb` file (e.g., `BA\\notebooks\\1_data_extraction\\reddit_scraper.ipynb`).\n",
    "#     *   Click on the file to open it in VS Code.\n",
    "#\n",
    "# 2.  **Execute Cells:**\n",
    "#     *   **Recommendation: Use \"Run All\".** While you can execute individual code cells by clicking the \"Play\" icon next to them or using `Shift + Enter`, it is **highly recommended to use the \"Run All\" option** (located at the top-right of the notebook window). This ensures that all cells are executed in the correct order, preventing potential issues with dependencies or state that might arise from executing cells individually or out of order.\n",
    "#     *   **Note on Markdown Cells:** When you run \"Run All\", this markdown cell (the guide itself) and the above will also be executed and displayed. You can easily collapse or hide its output by double-clicking on it. This will return it to its edit form, making the notebook cleaner for viewing. This is particularly useful for large informational cells like this one, which might otherwise appear prominently in the notebook output.\n",
    "#\n",
    "# 3.  **Expected Outputs within the Notebook:**\n",
    "#     *   **Logs:** `logging.info` messages will be displayed directly below the code cells.\n",
    "#     *   **Data Loading:** Checks will be performed to see if data is already present in the SQLite database. If so, it will load the existing data for further use. If not, data scraping processes will be initiated via `prepare_data.py` (though this step is primarily handled by the `prepare_data.py` script itself).\n",
    "#     *   **Plots:** Generated charts (e.g., correlation matrices, distribution plots) will be embedded directly within the output cells of the notebook via `plots.py`.\n",
    "#     *   **Statistical Test Results:** The results of statistical tests will be printed in the output cells via `statistical_tests.py`.\n",
    "#     *   **Confirmation Messages:** Messages like \"Combined DataFrame successfully created with shape: (X, Y)\" or \"Comments DataFrame loaded with shape: (X, Y)\" confirm progress.\n",
    "#\n",
    "# ##### **3.3 Step 3: Machine Learning Pipeline (`train_model.py`)**\n",
    "#\n",
    "# This script executes the machine learning workflow, including model training, hyperparameter tuning, evaluation, and saving of trained artifacts.\n",
    "#\n",
    "# 1.  **Open Terminal & Activate Environment:**\n",
    "#     *   Ensure you have a **VS Code Terminal** open and your virtual environment (`(venv)`) is active.\n",
    "#\n",
    "# 2.  **Execute Machine Learning Pipeline Script:**\n",
    "#     *   In the **VS Code Terminal**, run:\n",
    "#         ```bash\n",
    "#         python \"BA\\src\\models\\train_model.py\"\n",
    "#         ```\n",
    "#     *   **Expected Outputs in Terminal:**\n",
    "#         *   Detailed log messages about the progress of model training and evaluation.\n",
    "#         *   Performance metrics for Linear Regression and XGBoost (e.g., R² values, MSE).\n",
    "#         *   Information about hyperparameter tuning (best parameters).\n",
    "#         *   Messages regarding the generation and saving of SHAP plots (e.g., \"SHAP Summary Plot saved to: ...\").\n",
    "#         *   Confirmation of model artifact saving: \"Models and preprocessing objects saved to: ...\"\n",
    "#         *   Final completion message: \"Model Pipeline Completed Successfully.\"\n",
    "#\n",
    "# #### **4. Viewing Results**\n",
    "#\n",
    "# After successfully executing the project workflow, you can examine the generated outputs:\n",
    "#\n",
    "# 1.  **Explore Generated Files:**\n",
    "#     *   **Data:** The SQLite database (`reddit_dota2_analysis.db`) will be located in `BA\\data\\processed`. You can open it using SQLite viewers or directly within VS Code using the SQLite extension.\n",
    "#     *   **Plots & Visualizations:** All generated plots and visualizations will be saved in the `BA\\reports\\figures` directory. You can open these images using any image viewer.\n",
    "#     *   **Model Artifacts:** The trained models and preprocessing objects will be saved in the `BA\\models` directory. These can be loaded and used for inference or further analysis.\n",
    "#\n",
    "# 2.  **Logging:**\n",
    "#     *   All logs for `prepare_data.py` and the Notebook are saved in `BA\\logs\\model_pipeline.log`. This file contains detailed logs from both the data preparation and model training processes.\n",
    "#     *   A precise gathering of data outputs and visualizations for ML models can be also found in `BA\\reports\\figures`. Summaries of the model's performance and SHAP plots are saved here too.\n",
    "#\n",
    "# **End of Guide**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2b5e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-14 16:15:19,730 - INFO - data.pipeline - Starting data preparation and feature engineering pipeline...\n",
      "2025-08-14 16:15:19,731 - INFO - data.pipeline - --- Starting Data Preparation Pipeline ---\n",
      "2025-08-14 16:15:19,732 - INFO - data.pipeline - \n",
      "No processed data found in 'reddit_dota2_analysis.db'. Proceeding with data collection and processing.\n",
      "2025-08-14 16:15:19,734 - INFO - data.pipeline - Reddit instance initialized for subreddit: DotA2.\n",
      "2025-08-14 16:15:19,734 - INFO - data.pipeline - \n",
      "--- DATENSAMMLUNG ---\n",
      "2025-08-14 16:15:19,735 - INFO - data.pipeline - \n",
      "Collecting data for TI8 (Query: 'OG TI8')...\n",
      "2025-08-14 16:16:26,975 - INFO - data.pipeline - Successfully collected 736 comments for TI8.\n",
      "2025-08-14 16:16:26,975 - INFO - data.pipeline - --- Cleaning and Preprocessing for TI8 ---\n",
      "2025-08-14 16:16:29,148 - INFO - data.pipeline - Performing sentiment analysis and calculating text length...\n",
      "2025-08-14 16:16:29,266 - INFO - data.pipeline - Categorizing time periods and calculating days from event start...\n",
      "2025-08-14 16:16:29,276 - INFO - data.pipeline - --- Feature Engineering ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:BA.src.features.feature_engineering:No posts categorized as 'Ranking Update'. Consider reviewing keywords or data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-14 16:16:29,507 - INFO - data.pipeline - \n",
      "Collecting data for TI11 (Query: 'Tundra TI11')...\n",
      "2025-08-14 16:17:20,595 - INFO - data.pipeline - Successfully collected 649 comments for TI11.\n",
      "2025-08-14 16:17:20,596 - INFO - data.pipeline - --- Cleaning and Preprocessing for TI11 ---\n",
      "2025-08-14 16:17:20,671 - INFO - data.pipeline - Performing sentiment analysis and calculating text length...\n",
      "2025-08-14 16:17:20,767 - INFO - data.pipeline - Categorizing time periods and calculating days from event start...\n",
      "2025-08-14 16:17:20,775 - INFO - data.pipeline - --- Feature Engineering ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:BA.src.features.feature_engineering:No posts categorized as 'Ranking Update'. Consider reviewing keywords or data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-14 16:17:20,958 - INFO - data.pipeline - \n",
      "Collecting data for OG_RM24 (Query: 'OG')...\n",
      "2025-08-14 16:18:01,677 - INFO - data.pipeline - Successfully collected 750 comments for OG_RM24.\n",
      "2025-08-14 16:18:01,678 - INFO - data.pipeline - --- Cleaning and Preprocessing for OG_RM24 ---\n",
      "2025-08-14 16:18:01,761 - INFO - data.pipeline - Performing sentiment analysis and calculating text length...\n",
      "2025-08-14 16:18:02,134 - INFO - data.pipeline - Categorizing time periods and calculating days from event start...\n",
      "2025-08-14 16:18:02,143 - INFO - data.pipeline - --- Feature Engineering ---\n",
      "2025-08-14 16:18:02,361 - INFO - data.pipeline - \n",
      "Collecting data for Topson_RM24 (Query: 'Topson')...\n",
      "2025-08-14 16:18:41,397 - INFO - data.pipeline - Successfully collected 750 comments for Topson_RM24.\n",
      "2025-08-14 16:18:41,398 - INFO - data.pipeline - --- Cleaning and Preprocessing for Topson_RM24 ---\n",
      "2025-08-14 16:18:41,480 - INFO - data.pipeline - Performing sentiment analysis and calculating text length...\n",
      "2025-08-14 16:18:41,581 - INFO - data.pipeline - Categorizing time periods and calculating days from event start...\n",
      "2025-08-14 16:18:41,591 - INFO - data.pipeline - --- Feature Engineering ---\n",
      "2025-08-14 16:18:41,808 - INFO - data.pipeline - \n",
      "--- Saving processed data to SQLite database ---\n",
      "2025-08-14 16:18:41,909 - INFO - data.pipeline - \n",
      "--- Data storage in SQLite complete ---\n",
      "2025-08-14 16:18:41,910 - INFO - data.pipeline - --- Data Preparation Pipeline Completed ---\n",
      "2025-08-14 16:18:41,912 - INFO - data.pipeline - Data preparation and feature engineering pipeline complete.\n",
      "2025-08-14 16:18:41,912 - INFO - data.pipeline - Combined DataFrame successfully created with shape: (2636, 41)\n",
      "2025-08-14 16:18:41,912 - INFO - data.pipeline - Combined Cleaned DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2636 entries, 0 to 2635\n",
      "Data columns (total 41 columns):\n",
      " #   Column                       Non-Null Count  Dtype         \n",
      "---  ------                       --------------  -----         \n",
      " 0   post_id                      2636 non-null   object        \n",
      " 1   post_title                   2636 non-null   object        \n",
      " 2   post_url                     2636 non-null   object        \n",
      " 3   post_author                  2636 non-null   object        \n",
      " 4   post_created_utc             2636 non-null   datetime64[ns]\n",
      " 5   post_score                   2636 non-null   int64         \n",
      " 6   post_num_comments            2636 non-null   int64         \n",
      " 7   upvote_ratio                 2636 non-null   float64       \n",
      " 8   is_self                      2636 non-null   bool          \n",
      " 9   selftext                     2636 non-null   object        \n",
      " 10  link_flair_text              2598 non-null   object        \n",
      " 11  permalink                    2636 non-null   object        \n",
      " 12  comment_id                   2636 non-null   object        \n",
      " 13  comment_body                 2636 non-null   object        \n",
      " 14  comment_author               2636 non-null   object        \n",
      " 15  comment_created_utc          2636 non-null   datetime64[ns]\n",
      " 16  comment_score                2636 non-null   int64         \n",
      " 17  comment_permalink            2636 non-null   object        \n",
      " 18  processed_comment_body       2636 non-null   object        \n",
      " 19  neg_sentiment                2636 non-null   float64       \n",
      " 20  neu_sentiment                2636 non-null   float64       \n",
      " 21  pos_sentiment                2636 non-null   float64       \n",
      " 22  compound_sentiment           2636 non-null   float64       \n",
      " 23  char_count                   2636 non-null   int64         \n",
      " 24  word_count                   2636 non-null   int64         \n",
      " 25  time_period                  2636 non-null   object        \n",
      " 26  days_from_event_start        2636 non-null   int64         \n",
      " 27  post_title_length            2636 non-null   int64         \n",
      " 28  post_title_word_count        2636 non-null   int64         \n",
      " 29  contains_question            2636 non-null   bool          \n",
      " 30  author_karma                 2636 non-null   int64         \n",
      " 31  contains_team_name           2636 non-null   bool          \n",
      " 32  contains_player_keyword      2636 non-null   bool          \n",
      " 33  contains_hero_keyword        2636 non-null   bool          \n",
      " 34  contains_event_keyword       2636 non-null   bool          \n",
      " 35  post_type                    2636 non-null   object        \n",
      " 36  comment_to_post_score_ratio  2636 non-null   float64       \n",
      " 37  comment_score_per_day        2636 non-null   float64       \n",
      " 38  event_name                   2636 non-null   object        \n",
      " 39  comment_hour                 2636 non-null   int32         \n",
      " 40  comment_day_of_week          2636 non-null   object        \n",
      "dtypes: bool(6), datetime64[ns](2), float64(7), int32(1), int64(9), object(16)\n",
      "memory usage: 726.1+ KB\n",
      "\n",
      "Descriptive statistics of the combined DataFrame:\n",
      "        post_id                         post_title  \\\n",
      "count      2636                               2636   \n",
      "unique       45                                 45   \n",
      "top     1e10whq  I have a feeling he won’t listen    \n",
      "freq         98                                 98   \n",
      "mean        NaN                                NaN   \n",
      "min         NaN                                NaN   \n",
      "25%         NaN                                NaN   \n",
      "50%         NaN                                NaN   \n",
      "75%         NaN                                NaN   \n",
      "max         NaN                                NaN   \n",
      "std         NaN                                NaN   \n",
      "\n",
      "                                    post_url          post_author  \\\n",
      "count                                   2636                 2636   \n",
      "unique                                    45                   36   \n",
      "top     https://i.redd.it/qgwax0fsoybd1.jpeg  D2TournamentThreads   \n",
      "freq                                      98                  350   \n",
      "mean                                     NaN                  NaN   \n",
      "min                                      NaN                  NaN   \n",
      "25%                                      NaN                  NaN   \n",
      "50%                                      NaN                  NaN   \n",
      "75%                                      NaN                  NaN   \n",
      "max                                      NaN                  NaN   \n",
      "std                                      NaN                  NaN   \n",
      "\n",
      "                     post_created_utc    post_score  post_num_comments  \\\n",
      "count                            2636   2636.000000        2636.000000   \n",
      "unique                            NaN           NaN                NaN   \n",
      "top                               NaN           NaN                NaN   \n",
      "freq                              NaN           NaN                NaN   \n",
      "mean    2022-09-25 16:15:04.599013888   1946.131639         985.965478   \n",
      "min               2018-08-13 19:16:59    149.000000           8.000000   \n",
      "25%               2022-10-15 15:14:56    597.000000         116.000000   \n",
      "50%               2024-07-06 01:19:43   1775.000000         193.000000   \n",
      "75%               2024-07-11 21:57:41   2617.000000         358.000000   \n",
      "max               2024-07-26 10:32:20  20837.000000       13279.000000   \n",
      "std                               NaN   2651.232928        2446.594130   \n",
      "\n",
      "        upvote_ratio is_self selftext  ... contains_team_name  \\\n",
      "count    2636.000000    2636     2636  ...               2636   \n",
      "unique           NaN       2       24  ...                  2   \n",
      "top              NaN   False           ...              False   \n",
      "freq             NaN    1606     1606  ...               2255   \n",
      "mean        0.952439     NaN      NaN  ...                NaN   \n",
      "min         0.730000     NaN      NaN  ...                NaN   \n",
      "25%         0.940000     NaN      NaN  ...                NaN   \n",
      "50%         0.960000     NaN      NaN  ...                NaN   \n",
      "75%         0.980000     NaN      NaN  ...                NaN   \n",
      "max         0.990000     NaN      NaN  ...                NaN   \n",
      "std         0.039650     NaN      NaN  ...                NaN   \n",
      "\n",
      "       contains_player_keyword contains_hero_keyword contains_event_keyword  \\\n",
      "count                     2636                  2636                   2636   \n",
      "unique                       2                     2                      2   \n",
      "top                      False                 False                  False   \n",
      "freq                      2079                  2075                   1782   \n",
      "mean                       NaN                   NaN                    NaN   \n",
      "min                        NaN                   NaN                    NaN   \n",
      "25%                        NaN                   NaN                    NaN   \n",
      "50%                        NaN                   NaN                    NaN   \n",
      "75%                        NaN                   NaN                    NaN   \n",
      "max                        NaN                   NaN                    NaN   \n",
      "std                        NaN                   NaN                    NaN   \n",
      "\n",
      "              post_type comment_to_post_score_ratio  comment_score_per_day  \\\n",
      "count              2636                 2636.000000           2.636000e+03   \n",
      "unique                4                         NaN                    NaN   \n",
      "top     Player Transfer                         NaN                    NaN   \n",
      "freq               1219                         NaN                    NaN   \n",
      "mean                NaN                    0.052148           1.178796e+04   \n",
      "min                 NaN                   -0.064655          -1.959885e+02   \n",
      "25%                 NaN                    0.006958           4.967302e+01   \n",
      "50%                 NaN                    0.017462           1.868628e+02   \n",
      "75%                 NaN                    0.056618           7.502051e+02   \n",
      "max                 NaN                    1.943289           2.442764e+06   \n",
      "std                 NaN                    0.107329           1.012669e+05   \n",
      "\n",
      "       event_name comment_hour  comment_day_of_week  \n",
      "count        2636  2636.000000                 2636  \n",
      "unique          4          NaN                    7  \n",
      "top       OG_RM24          NaN               Sunday  \n",
      "freq          705          NaN                  527  \n",
      "mean          NaN    11.041351                  NaN  \n",
      "min           NaN     0.000000                  NaN  \n",
      "25%           NaN     5.000000                  NaN  \n",
      "50%           NaN    11.000000                  NaN  \n",
      "75%           NaN    17.000000                  NaN  \n",
      "max           NaN    23.000000                  NaN  \n",
      "std           NaN     6.853204                  NaN  \n",
      "\n",
      "[11 rows x 41 columns]\n",
      "2025-08-14 16:18:41,955 - INFO - data.pipeline - \n",
      "--- Loading data from SQLite database ---\n",
      "2025-08-14 16:18:41,956 - INFO - data.pipeline - Comments DataFrame loaded with shape: (2636, 41)\n",
      "2025-08-14 16:18:41,956 - INFO - data.pipeline - \n",
      "Comments DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2636 entries, 0 to 2635\n",
      "Data columns (total 41 columns):\n",
      " #   Column                       Non-Null Count  Dtype         \n",
      "---  ------                       --------------  -----         \n",
      " 0   post_id                      2636 non-null   object        \n",
      " 1   post_title                   2636 non-null   object        \n",
      " 2   post_url                     2636 non-null   object        \n",
      " 3   post_author                  2636 non-null   object        \n",
      " 4   post_created_utc             2636 non-null   datetime64[ns]\n",
      " 5   post_score                   2636 non-null   int64         \n",
      " 6   post_num_comments            2636 non-null   int64         \n",
      " 7   upvote_ratio                 2636 non-null   float64       \n",
      " 8   is_self                      2636 non-null   bool          \n",
      " 9   selftext                     2636 non-null   object        \n",
      " 10  link_flair_text              2598 non-null   object        \n",
      " 11  permalink                    2636 non-null   object        \n",
      " 12  comment_id                   2636 non-null   object        \n",
      " 13  comment_body                 2636 non-null   object        \n",
      " 14  comment_author               2636 non-null   object        \n",
      " 15  comment_created_utc          2636 non-null   datetime64[ns]\n",
      " 16  comment_score                2636 non-null   int64         \n",
      " 17  comment_permalink            2636 non-null   object        \n",
      " 18  processed_comment_body       2636 non-null   object        \n",
      " 19  neg_sentiment                2636 non-null   float64       \n",
      " 20  neu_sentiment                2636 non-null   float64       \n",
      " 21  pos_sentiment                2636 non-null   float64       \n",
      " 22  compound_sentiment           2636 non-null   float64       \n",
      " 23  char_count                   2636 non-null   int64         \n",
      " 24  word_count                   2636 non-null   int64         \n",
      " 25  time_period                  2636 non-null   object        \n",
      " 26  days_from_event_start        2636 non-null   int64         \n",
      " 27  post_title_length            2636 non-null   int64         \n",
      " 28  post_title_word_count        2636 non-null   int64         \n",
      " 29  contains_question            2636 non-null   bool          \n",
      " 30  author_karma                 2636 non-null   int64         \n",
      " 31  contains_team_name           2636 non-null   bool          \n",
      " 32  contains_player_keyword      2636 non-null   bool          \n",
      " 33  contains_hero_keyword        2636 non-null   bool          \n",
      " 34  contains_event_keyword       2636 non-null   bool          \n",
      " 35  post_type                    2636 non-null   object        \n",
      " 36  comment_to_post_score_ratio  2636 non-null   float64       \n",
      " 37  comment_score_per_day        2636 non-null   float64       \n",
      " 38  event_name                   2636 non-null   object        \n",
      " 39  comment_hour                 2636 non-null   int32         \n",
      " 40  comment_day_of_week          2636 non-null   object        \n",
      "dtypes: bool(6), datetime64[ns](2), float64(7), int32(1), int64(9), object(16)\n",
      "memory usage: 726.1+ KB\n",
      "\n",
      "Descriptive statistics of the loaded DataFrame:\n",
      "        post_id                         post_title  \\\n",
      "count      2636                               2636   \n",
      "unique       45                                 45   \n",
      "top     1e10whq  I have a feeling he won’t listen    \n",
      "freq         98                                 98   \n",
      "mean        NaN                                NaN   \n",
      "min         NaN                                NaN   \n",
      "25%         NaN                                NaN   \n",
      "50%         NaN                                NaN   \n",
      "75%         NaN                                NaN   \n",
      "max         NaN                                NaN   \n",
      "std         NaN                                NaN   \n",
      "\n",
      "                                    post_url          post_author  \\\n",
      "count                                   2636                 2636   \n",
      "unique                                    45                   36   \n",
      "top     https://i.redd.it/qgwax0fsoybd1.jpeg  D2TournamentThreads   \n",
      "freq                                      98                  350   \n",
      "mean                                     NaN                  NaN   \n",
      "min                                      NaN                  NaN   \n",
      "25%                                      NaN                  NaN   \n",
      "50%                                      NaN                  NaN   \n",
      "75%                                      NaN                  NaN   \n",
      "max                                      NaN                  NaN   \n",
      "std                                      NaN                  NaN   \n",
      "\n",
      "                     post_created_utc    post_score  post_num_comments  \\\n",
      "count                            2636   2636.000000        2636.000000   \n",
      "unique                            NaN           NaN                NaN   \n",
      "top                               NaN           NaN                NaN   \n",
      "freq                              NaN           NaN                NaN   \n",
      "mean    2022-09-25 16:15:04.599013888   1946.131639         985.965478   \n",
      "min               2018-08-13 19:16:59    149.000000           8.000000   \n",
      "25%               2022-10-15 15:14:56    597.000000         116.000000   \n",
      "50%               2024-07-06 01:19:43   1775.000000         193.000000   \n",
      "75%               2024-07-11 21:57:41   2617.000000         358.000000   \n",
      "max               2024-07-26 10:32:20  20837.000000       13279.000000   \n",
      "std                               NaN   2651.232928        2446.594130   \n",
      "\n",
      "        upvote_ratio is_self selftext  ... contains_team_name  \\\n",
      "count    2636.000000    2636     2636  ...               2636   \n",
      "unique           NaN       2       24  ...                  2   \n",
      "top              NaN   False           ...              False   \n",
      "freq             NaN    1606     1606  ...               2255   \n",
      "mean        0.952439     NaN      NaN  ...                NaN   \n",
      "min         0.730000     NaN      NaN  ...                NaN   \n",
      "25%         0.940000     NaN      NaN  ...                NaN   \n",
      "50%         0.960000     NaN      NaN  ...                NaN   \n",
      "75%         0.980000     NaN      NaN  ...                NaN   \n",
      "max         0.990000     NaN      NaN  ...                NaN   \n",
      "std         0.039650     NaN      NaN  ...                NaN   \n",
      "\n",
      "       contains_player_keyword contains_hero_keyword contains_event_keyword  \\\n",
      "count                     2636                  2636                   2636   \n",
      "unique                       2                     2                      2   \n",
      "top                      False                 False                  False   \n",
      "freq                      2079                  2075                   1782   \n",
      "mean                       NaN                   NaN                    NaN   \n",
      "min                        NaN                   NaN                    NaN   \n",
      "25%                        NaN                   NaN                    NaN   \n",
      "50%                        NaN                   NaN                    NaN   \n",
      "75%                        NaN                   NaN                    NaN   \n",
      "max                        NaN                   NaN                    NaN   \n",
      "std                        NaN                   NaN                    NaN   \n",
      "\n",
      "              post_type comment_to_post_score_ratio  comment_score_per_day  \\\n",
      "count              2636                 2636.000000           2.636000e+03   \n",
      "unique                4                         NaN                    NaN   \n",
      "top     Player Transfer                         NaN                    NaN   \n",
      "freq               1219                         NaN                    NaN   \n",
      "mean                NaN                    0.052148           1.178796e+04   \n",
      "min                 NaN                   -0.064655          -1.959885e+02   \n",
      "25%                 NaN                    0.006958           4.967302e+01   \n",
      "50%                 NaN                    0.017462           1.868628e+02   \n",
      "75%                 NaN                    0.056618           7.502051e+02   \n",
      "max                 NaN                    1.943289           2.442764e+06   \n",
      "std                 NaN                    0.107329           1.012669e+05   \n",
      "\n",
      "       event_name comment_hour  comment_day_of_week  \n",
      "count        2636  2636.000000                 2636  \n",
      "unique          4          NaN                    7  \n",
      "top       OG_RM24          NaN               Sunday  \n",
      "freq          705          NaN                  527  \n",
      "mean          NaN    11.041351                  NaN  \n",
      "min           NaN     0.000000                  NaN  \n",
      "25%           NaN     5.000000                  NaN  \n",
      "50%           NaN    11.000000                  NaN  \n",
      "75%           NaN    17.000000                  NaN  \n",
      "max           NaN    23.000000                  NaN  \n",
      "std           NaN     6.853204                  NaN  \n",
      "\n",
      "[11 rows x 41 columns]\n",
      "2025-08-14 16:18:41,997 - INFO - eda.stats - \n",
      "--- Performing Statistical Tests ---\n",
      "\n",
      "--- Independent Samples t-test for Compound Sentiment (TI8 vs. TI11) ---\n",
      "2025-08-14 16:18:41,997 - INFO - eda.stats - \n",
      "--- Performing Independent Samples t-test for 'compound_sentiment' between 'TI8' and 'TI11' ---\n",
      "2025-08-14 16:18:42,002 - INFO - eda.stats -   t-statistic: 1.458\n",
      "2025-08-14 16:18:42,003 - INFO - eda.stats -   p-value: 0.145\n",
      "2025-08-14 16:18:42,003 - INFO - eda.stats -   Interpretation: There is no statistically significant difference (p=0.145) in 'compound_sentiment' between 'TI8' (Mean: 0.23) and 'TI11' (Mean: 0.19).\n",
      "\n",
      "--- Independent Samples t-test for Compound Sentiment (TI8 vs. OG_RM24) ---\n",
      "2025-08-14 16:18:42,003 - INFO - eda.stats - \n",
      "--- Performing Independent Samples t-test for 'compound_sentiment' between 'TI8' and 'OG_RM24' ---\n",
      "2025-08-14 16:18:42,007 - INFO - eda.stats -   t-statistic: 2.847\n",
      "2025-08-14 16:18:42,007 - INFO - eda.stats -   p-value: 0.004\n",
      "2025-08-14 16:18:42,008 - INFO - eda.stats -   Interpretation: There is a statistically significant difference (p=0.004) in 'compound_sentiment' between 'TI8' (Mean: 0.23) and 'OG_RM24' (Mean: 0.16).\n",
      "\n",
      "--- Independent Samples t-test for Compound Sentiment (TI11 vs. OG_RM24) ---\n",
      "2025-08-14 16:18:42,008 - INFO - eda.stats - \n",
      "--- Performing Independent Samples t-test for 'compound_sentiment' between 'TI11' and 'OG_RM24' ---\n",
      "2025-08-14 16:18:42,012 - INFO - eda.stats -   t-statistic: 1.264\n",
      "2025-08-14 16:18:42,012 - INFO - eda.stats -   p-value: 0.206\n",
      "2025-08-14 16:18:42,013 - INFO - eda.stats -   Interpretation: There is no statistically significant difference (p=0.206) in 'compound_sentiment' between 'TI11' (Mean: 0.19) and 'OG_RM24' (Mean: 0.16).\n",
      "\n",
      "--- Independent Samples t-test for Compound Sentiment (OG_RM24 vs. Topson_RM24) ---\n",
      "2025-08-14 16:18:42,013 - INFO - eda.stats - \n",
      "--- Performing Independent Samples t-test for 'compound_sentiment' between 'OG_RM24' and 'Topson_RM24' ---\n",
      "2025-08-14 16:18:42,019 - INFO - eda.stats -   t-statistic: 0.009\n",
      "2025-08-14 16:18:42,020 - INFO - eda.stats -   p-value: 0.993\n",
      "2025-08-14 16:18:42,020 - INFO - eda.stats -   Interpretation: There is no statistically significant difference (p=0.993) in 'compound_sentiment' between 'OG_RM24' (Mean: 0.16) and 'Topson_RM24' (Mean: 0.16).\n",
      "\n",
      "--- One-Way ANOVA for Comment Score across Time Periods ---\n",
      "2025-08-14 16:18:42,021 - INFO - eda.stats - \n",
      "--- Performing One-Way ANOVA test for 'comment_score' across groups in 'time_period' ---\n",
      "2025-08-14 16:18:42,027 - INFO - eda.stats -   F-statistic: 0.672\n",
      "2025-08-14 16:18:42,028 - INFO - eda.stats -   p-value: 0.511\n",
      "2025-08-14 16:18:42,028 - INFO - eda.stats -   Interpretation: There is no statistically significant difference (p=0.511) in 'comment_score' across groups in 'time_period'.\n",
      "\n",
      "--- One-Way ANOVA for Compound Sentiment across Time Periods ---\n",
      "2025-08-14 16:18:42,029 - INFO - eda.stats - \n",
      "--- Performing One-Way ANOVA test for 'compound_sentiment' across groups in 'time_period' ---\n",
      "2025-08-14 16:18:42,036 - INFO - eda.stats -   F-statistic: 1.353\n",
      "2025-08-14 16:18:42,036 - INFO - eda.stats -   p-value: 0.259\n",
      "2025-08-14 16:18:42,037 - INFO - eda.stats -   Interpretation: There is no statistically significant difference (p=0.259) in 'compound_sentiment' across groups in 'time_period'.\n",
      "\n",
      "--- One-Way ANOVA for Comment Score across Post Types ---\n",
      "2025-08-14 16:18:42,037 - INFO - eda.stats - \n",
      "--- Performing One-Way ANOVA test for 'comment_score' across groups in 'post_type' ---\n",
      "2025-08-14 16:18:42,043 - INFO - eda.stats -   F-statistic: 5.918\n",
      "2025-08-14 16:18:42,044 - INFO - eda.stats -   p-value: 0.001\n",
      "2025-08-14 16:18:42,045 - INFO - eda.stats -   Interpretation: There is a statistically significant difference (p=0.001) in 'comment_score' across at least two groups in 'post_type'.\n",
      "\n",
      "--- One-Way ANOVA for Compound Sentiment across Post Types ---\n",
      "2025-08-14 16:18:42,045 - INFO - eda.stats - \n",
      "--- Performing One-Way ANOVA test for 'compound_sentiment' across groups in 'post_type' ---\n",
      "2025-08-14 16:18:42,052 - INFO - eda.stats -   F-statistic: 3.325\n",
      "2025-08-14 16:18:42,053 - INFO - eda.stats -   p-value: 0.019\n",
      "2025-08-14 16:18:42,054 - INFO - eda.stats -   Interpretation: There is a statistically significant difference (p=0.019) in 'compound_sentiment' across at least two groups in 'post_type'.\n",
      "\n",
      "--- Chi-squared test for Time Period and Contains Question ---\n",
      "2025-08-14 16:18:42,054 - INFO - eda.stats - \n",
      "--- Performing Chi-squared test of independence between 'time_period' and 'contains_question' ---\n",
      "2025-08-14 16:18:42,060 - INFO - eda.stats -   Chi-squared statistic: 1.454\n",
      "2025-08-14 16:18:42,061 - INFO - eda.stats -   p-value: 0.483\n",
      "2025-08-14 16:18:42,061 - INFO - eda.stats -   Degrees of freedom: 2\n",
      "2025-08-14 16:18:42,062 - INFO - eda.stats -   Interpretation: There is no statistically significant association (p=0.483) between 'time_period' and 'contains_question'. This suggests that the two variables are independent.\n",
      "\n",
      "--- Chi-squared test for Event Name and Contains Team Name ---\n",
      "2025-08-14 16:18:42,062 - INFO - eda.stats - \n",
      "--- Performing Chi-squared test of independence between 'event_name' and 'contains_team_name' ---\n",
      "2025-08-14 16:18:42,071 - INFO - eda.stats -   Chi-squared statistic: 311.769\n",
      "2025-08-14 16:18:42,072 - INFO - eda.stats -   p-value: 0.000\n",
      "2025-08-14 16:18:42,073 - INFO - eda.stats -   Degrees of freedom: 3\n",
      "2025-08-14 16:18:42,073 - INFO - eda.stats -   Interpretation: There is a statistically significant association (p=0.000) between 'event_name' and 'contains_team_name'. This suggests that the two variables are not independent.\n",
      "2025-08-14 16:18:42,073 - INFO - eda.stats - \n",
      "--- Statistical Tests Complete ---\n",
      "2025-08-14 16:18:42,074 - INFO - eda.stats - \n",
      "--- Generating Exploratory Data Analysis Plots ---\n",
      "2025-08-14 16:18:42,074 - INFO - eda.stats - \n",
      "--- Generating Feature Distribution Plots ---\n",
      "2025-08-14 16:18:42,077 - INFO - eda.stats - Table data saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\final_plots\\RQ5_dist_comment_score_comment_score_summary.txt\n",
      "2025-08-14 16:18:42,091 - INFO - eda.stats - Table data saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\final_plots\\RQ5_dist_comment_score_comment_score_kde_curve.txt\n",
      "2025-08-14 16:18:42,603 - INFO - eda.stats - Plot saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\figures\\final_plots\\RQ5_dist_comment_score_comment_score_histogram_extended.png\n",
      "2025-08-14 16:18:42,607 - INFO - eda.stats - Table data saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\other_eda_plots\\dist_sentiment_compound_sentiment_summary.txt\n",
      "2025-08-14 16:18:42,630 - INFO - eda.stats - Table data saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\other_eda_plots\\dist_sentiment_compound_sentiment_kde_curve.txt\n",
      "2025-08-14 16:18:43,064 - INFO - eda.stats - Plot saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\figures\\other_eda_plots\\dist_sentiment_compound_sentiment_histogram_extended.png\n",
      "2025-08-14 16:18:43,067 - INFO - eda.stats - Table data saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\other_eda_plots\\dist_word_count_word_count_summary.txt\n",
      "2025-08-14 16:18:43,084 - INFO - eda.stats - Table data saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\other_eda_plots\\dist_word_count_word_count_kde_curve.txt\n",
      "2025-08-14 16:18:43,554 - INFO - eda.stats - Plot saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\figures\\other_eda_plots\\dist_word_count_word_count_histogram_extended.png\n",
      "2025-08-14 16:18:43,555 - INFO - eda.stats - \n",
      "--- Generating Distribution Comparison Plots (Violin/Box) ---\n",
      "2025-08-14 16:18:43,562 - INFO - eda.stats - Table data saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\final_plots\\RQ4_dist_comp_word_count_word_count_violin_stats.txt\n",
      "2025-08-14 16:18:43,833 - INFO - eda.stats - Plot saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\figures\\final_plots\\RQ4_dist_comp_word_count_word_count_violin_extended.png\n",
      "2025-08-14 16:18:43,840 - INFO - eda.stats - Table data saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\other_eda_plots\\dist_comp_sentiment_compound_sentiment_violin_stats.txt\n",
      "2025-08-14 16:18:44,118 - INFO - eda.stats - Plot saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\figures\\other_eda_plots\\dist_comp_sentiment_compound_sentiment_violin_extended.png\n",
      "2025-08-14 16:18:44,125 - INFO - eda.stats - Table data saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\other_eda_plots\\dist_comp_comment_score_comment_score_violin_stats.txt\n",
      "2025-08-14 16:18:44,376 - INFO - eda.stats - Plot saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\figures\\other_eda_plots\\dist_comp_comment_score_comment_score_violin_extended.png\n",
      "2025-08-14 16:18:44,377 - INFO - eda.stats - \n",
      "--- Generating Time Period Analysis Plots ---\n",
      "2025-08-14 16:18:44,443 - INFO - eda.stats - Table data saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\final_plots\\RQ3_comment_distribution_time_periods_table.txt\n",
      "2025-08-14 16:18:44,700 - INFO - eda.stats - Plot saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\figures\\final_plots\\RQ3_comment_distribution_time_periods.png\n",
      "2025-08-14 16:18:44,710 - INFO - eda.stats - Table data saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\final_plots\\RQ3_avg_comment_score_by_time_period_and_event.txt\n",
      "2025-08-14 16:18:44,987 - INFO - eda.stats - Plot saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\figures\\final_plots\\RQ3_avg_comment_score_by_time_period_and_event_extended.png\n",
      "2025-08-14 16:18:44,997 - INFO - eda.stats - Table data saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\other_eda_plots\\avg_sentiment_by_time_period_and_event.txt\n",
      "2025-08-14 16:18:45,289 - INFO - eda.stats - Plot saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\figures\\other_eda_plots\\avg_sentiment_by_time_period_and_event_extended.png\n",
      "2025-08-14 16:18:45,290 - INFO - eda.stats - \n",
      "--- Generating Engagement by Post Type Plots ---\n",
      "2025-08-14 16:18:45,301 - INFO - eda.stats - Table data saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\final_plots\\RQ1_comment_score_by_post_type_and_event.txt\n",
      "2025-08-14 16:18:45,661 - INFO - eda.stats - Plot saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\figures\\final_plots\\RQ1_avg_comment_score_by_post_type.png\n",
      "2025-08-14 16:18:45,670 - INFO - eda.stats - Table data saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\final_plots\\RQ1_compound_sentiment_by_post_type_and_event.txt\n",
      "2025-08-14 16:18:46,029 - INFO - eda.stats - Plot saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\figures\\final_plots\\RQ1_avg_sentiment_by_post_type.png\n",
      "2025-08-14 16:18:46,039 - INFO - eda.stats - Table data saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\other_eda_plots\\word_count_by_post_type_and_event.txt\n",
      "2025-08-14 16:18:46,401 - INFO - eda.stats - Plot saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\figures\\other_eda_plots\\avg_word_count_by_post_type.png\n",
      "2025-08-14 16:18:46,411 - INFO - eda.stats - Table data saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\final_plots\\RQ2_word_count_by_post_type_and_event.txt\n",
      "2025-08-14 16:18:46,766 - INFO - eda.stats - Plot saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\figures\\final_plots\\RQ2_filtered_avg_word_count_by_post_type_and_event.png\n",
      "2025-08-14 16:18:46,767 - INFO - eda.stats - \n",
      "--- Generating Pearson Correlation Matrix ---\n",
      "2025-08-14 16:18:46,768 - INFO - eda.stats - \n",
      "--- Calculating Pearson Correlations ---\n",
      "2025-08-14 16:18:46,771 - INFO - eda.stats - Correlation Matrix:\n",
      "                         contains_hero_keyword  comment_score_per_day  \\\n",
      "contains_hero_keyword                 1.000000               0.013425   \n",
      "comment_score_per_day                 0.013425               1.000000   \n",
      "char_count                            0.249623              -0.008363   \n",
      "post_title_length                     0.013067               0.018754   \n",
      "upvote_ratio                          0.100570              -0.116627   \n",
      "comment_score                        -0.009456               0.689869   \n",
      "author_karma                               NaN                    NaN   \n",
      "contains_event_keyword               -0.015350              -0.005100   \n",
      "days_from_event_start                -0.041589               0.083042   \n",
      "post_title_word_count                 0.039083              -0.010320   \n",
      "pos_sentiment                        -0.047912               0.003548   \n",
      "post_score                            0.024429               0.518475   \n",
      "contains_team_name                    0.012954               0.017387   \n",
      "contains_question                     0.000072               0.001606   \n",
      "word_count                            0.230113              -0.004902   \n",
      "is_self                              -0.082077               0.079011   \n",
      "neg_sentiment                         0.026861               0.034441   \n",
      "neu_sentiment                         0.030099              -0.026375   \n",
      "compound_sentiment                    0.012939              -0.005845   \n",
      "post_num_comments                    -0.050758               0.079878   \n",
      "contains_player_keyword              -0.019394               0.074161   \n",
      "comment_hour                         -0.040743              -0.048109   \n",
      "\n",
      "                         char_count  post_title_length  upvote_ratio  \\\n",
      "contains_hero_keyword      0.249623           0.013067      0.100570   \n",
      "comment_score_per_day     -0.008363           0.018754     -0.116627   \n",
      "char_count                 1.000000           0.043359     -0.044904   \n",
      "post_title_length          0.043359           1.000000      0.100618   \n",
      "upvote_ratio              -0.044904           0.100618      1.000000   \n",
      "comment_score             -0.008935          -0.003247     -0.096636   \n",
      "author_karma                    NaN                NaN           NaN   \n",
      "contains_event_keyword     0.342949           0.035846     -0.095353   \n",
      "days_from_event_start      0.118876           0.154445     -0.210114   \n",
      "post_title_word_count      0.043066           0.974666      0.143128   \n",
      "pos_sentiment              0.043326           0.053291      0.014307   \n",
      "post_score                 0.028350           0.018522     -0.117638   \n",
      "contains_team_name         0.222590          -0.083491     -0.052779   \n",
      "contains_question          0.053504           0.019439     -0.004046   \n",
      "word_count                 0.962204           0.048185     -0.048437   \n",
      "is_self                    0.091746          -0.253281     -0.188137   \n",
      "neg_sentiment              0.037080           0.016153     -0.043770   \n",
      "neu_sentiment             -0.057576          -0.052862      0.022766   \n",
      "compound_sentiment         0.245549           0.032492      0.008439   \n",
      "post_num_comments          0.014498          -0.162065      0.097037   \n",
      "contains_player_keyword    0.236574          -0.060561     -0.037006   \n",
      "comment_hour              -0.013371          -0.003633     -0.085481   \n",
      "\n",
      "                         comment_score  author_karma  contains_event_keyword  \\\n",
      "contains_hero_keyword        -0.009456           NaN               -0.015350   \n",
      "comment_score_per_day         0.689869           NaN               -0.005100   \n",
      "char_count                   -0.008935           NaN                0.342949   \n",
      "post_title_length            -0.003247           NaN                0.035846   \n",
      "upvote_ratio                 -0.096636           NaN               -0.095353   \n",
      "comment_score                 1.000000           NaN               -0.016079   \n",
      "author_karma                       NaN           NaN                     NaN   \n",
      "contains_event_keyword       -0.016079           NaN                1.000000   \n",
      "days_from_event_start         0.079050           NaN                0.171516   \n",
      "post_title_word_count        -0.034242           NaN                0.022331   \n",
      "pos_sentiment                -0.029640           NaN                0.104813   \n",
      "post_score                    0.551125           NaN               -0.053855   \n",
      "contains_team_name            0.010578           NaN                0.284855   \n",
      "contains_question            -0.012768           NaN                0.026592   \n",
      "word_count                   -0.001374           NaN                0.370897   \n",
      "is_self                       0.045195           NaN                0.244731   \n",
      "neg_sentiment                 0.022174           NaN                0.014080   \n",
      "neu_sentiment                 0.011281           NaN               -0.096192   \n",
      "compound_sentiment           -0.009710           NaN                0.185845   \n",
      "post_num_comments             0.137127           NaN                0.111753   \n",
      "contains_player_keyword       0.068462           NaN                0.273120   \n",
      "comment_hour                 -0.058535           NaN                0.034745   \n",
      "\n",
      "                         days_from_event_start  post_title_word_count  ...  \\\n",
      "contains_hero_keyword                -0.041589               0.039083  ...   \n",
      "comment_score_per_day                 0.083042              -0.010320  ...   \n",
      "char_count                            0.118876               0.043066  ...   \n",
      "post_title_length                     0.154445               0.974666  ...   \n",
      "upvote_ratio                         -0.210114               0.143128  ...   \n",
      "comment_score                         0.079050              -0.034242  ...   \n",
      "author_karma                               NaN                    NaN  ...   \n",
      "contains_event_keyword                0.171516               0.022331  ...   \n",
      "days_from_event_start                 1.000000               0.109458  ...   \n",
      "post_title_word_count                 0.109458               1.000000  ...   \n",
      "pos_sentiment                         0.082616               0.050507  ...   \n",
      "post_score                            0.117145              -0.031709  ...   \n",
      "contains_team_name                    0.149844              -0.090318  ...   \n",
      "contains_question                     0.015114               0.019422  ...   \n",
      "word_count                            0.116854               0.046085  ...   \n",
      "is_self                               0.199573              -0.235143  ...   \n",
      "neg_sentiment                         0.050128               0.018671  ...   \n",
      "neu_sentiment                        -0.108967              -0.051728  ...   \n",
      "compound_sentiment                    0.094148               0.030650  ...   \n",
      "post_num_comments                     0.311044              -0.177954  ...   \n",
      "contains_player_keyword               0.252677              -0.075958  ...   \n",
      "comment_hour                          0.040940              -0.025355  ...   \n",
      "\n",
      "                         contains_team_name  contains_question  word_count  \\\n",
      "contains_hero_keyword              0.012954           0.000072    0.230113   \n",
      "comment_score_per_day              0.017387           0.001606   -0.004902   \n",
      "char_count                         0.222590           0.053504    0.962204   \n",
      "post_title_length                 -0.083491           0.019439    0.048185   \n",
      "upvote_ratio                      -0.052779          -0.004046   -0.048437   \n",
      "comment_score                      0.010578          -0.012768   -0.001374   \n",
      "author_karma                            NaN                NaN         NaN   \n",
      "contains_event_keyword             0.284855           0.026592    0.370897   \n",
      "days_from_event_start              0.149844           0.015114    0.116854   \n",
      "post_title_word_count             -0.090318           0.019422    0.046085   \n",
      "pos_sentiment                      0.061626          -0.032030    0.058174   \n",
      "post_score                        -0.085884          -0.023362    0.028730   \n",
      "contains_team_name                 1.000000           0.037488    0.244857   \n",
      "contains_question                  0.037488           1.000000    0.036269   \n",
      "word_count                         0.244857           0.036269    1.000000   \n",
      "is_self                            0.296574           0.033476    0.101903   \n",
      "neg_sentiment                      0.026180          -0.013281    0.037088   \n",
      "neu_sentiment                     -0.068764           0.041661   -0.071548   \n",
      "compound_sentiment                 0.110689           0.008843    0.259055   \n",
      "post_num_comments                  0.123486           0.010605    0.005737   \n",
      "contains_player_keyword            0.691058           0.021622    0.255308   \n",
      "comment_hour                       0.027908           0.013093   -0.028515   \n",
      "\n",
      "                          is_self  neg_sentiment  neu_sentiment  \\\n",
      "contains_hero_keyword   -0.082077       0.026861       0.030099   \n",
      "comment_score_per_day    0.079011       0.034441      -0.026375   \n",
      "char_count               0.091746       0.037080      -0.057576   \n",
      "post_title_length       -0.253281       0.016153      -0.052862   \n",
      "upvote_ratio            -0.188137      -0.043770       0.022766   \n",
      "comment_score            0.045195       0.022174       0.011281   \n",
      "author_karma                  NaN            NaN            NaN   \n",
      "contains_event_keyword   0.244731       0.014080      -0.096192   \n",
      "days_from_event_start    0.199573       0.050128      -0.108967   \n",
      "post_title_word_count   -0.235143       0.018671      -0.051728   \n",
      "pos_sentiment            0.041982      -0.231923      -0.726508   \n",
      "post_score              -0.050605       0.032344      -0.024958   \n",
      "contains_team_name       0.296574       0.026180      -0.068764   \n",
      "contains_question        0.033476      -0.013281       0.041661   \n",
      "word_count               0.101903       0.037088      -0.071548   \n",
      "is_self                  1.000000       0.056560      -0.067791   \n",
      "neg_sentiment            0.056560       1.000000      -0.469176   \n",
      "neu_sentiment           -0.067791      -0.469176       1.000000   \n",
      "compound_sentiment       0.051820      -0.596636      -0.195281   \n",
      "post_num_comments        0.406938       0.024508      -0.038724   \n",
      "contains_player_keyword  0.345407       0.013913      -0.045632   \n",
      "comment_hour             0.117492      -0.020864       0.026244   \n",
      "\n",
      "                         compound_sentiment  post_num_comments  \\\n",
      "contains_hero_keyword              0.012939          -0.050758   \n",
      "comment_score_per_day             -0.005845           0.079878   \n",
      "char_count                         0.245549           0.014498   \n",
      "post_title_length                  0.032492          -0.162065   \n",
      "upvote_ratio                       0.008439           0.097037   \n",
      "comment_score                     -0.009710           0.137127   \n",
      "author_karma                            NaN                NaN   \n",
      "contains_event_keyword             0.185845           0.111753   \n",
      "days_from_event_start              0.094148           0.311044   \n",
      "post_title_word_count              0.030650          -0.177954   \n",
      "pos_sentiment                      0.678247           0.028589   \n",
      "post_score                         0.003141           0.138416   \n",
      "contains_team_name                 0.110689           0.123486   \n",
      "contains_question                  0.008843           0.010605   \n",
      "word_count                         0.259055           0.005737   \n",
      "is_self                            0.051820           0.406938   \n",
      "neg_sentiment                     -0.596636           0.024508   \n",
      "neu_sentiment                     -0.195281          -0.038724   \n",
      "compound_sentiment                 1.000000           0.022121   \n",
      "post_num_comments                  0.022121           1.000000   \n",
      "contains_player_keyword            0.107901           0.264598   \n",
      "comment_hour                       0.002795           0.064975   \n",
      "\n",
      "                         contains_player_keyword  comment_hour  \n",
      "contains_hero_keyword                  -0.019394     -0.040743  \n",
      "comment_score_per_day                   0.074161     -0.048109  \n",
      "char_count                              0.236574     -0.013371  \n",
      "post_title_length                      -0.060561     -0.003633  \n",
      "upvote_ratio                           -0.037006     -0.085481  \n",
      "comment_score                           0.068462     -0.058535  \n",
      "author_karma                                 NaN           NaN  \n",
      "contains_event_keyword                  0.273120      0.034745  \n",
      "days_from_event_start                   0.252677      0.040940  \n",
      "post_title_word_count                  -0.075958     -0.025355  \n",
      "pos_sentiment                           0.038130     -0.007811  \n",
      "post_score                             -0.001368     -0.134009  \n",
      "contains_team_name                      0.691058      0.027908  \n",
      "contains_question                       0.021622      0.013093  \n",
      "word_count                              0.255308     -0.028515  \n",
      "is_self                                 0.345407      0.117492  \n",
      "neg_sentiment                           0.013913     -0.020864  \n",
      "neu_sentiment                          -0.045632      0.026244  \n",
      "compound_sentiment                      0.107901      0.002795  \n",
      "post_num_comments                       0.264598      0.064975  \n",
      "contains_player_keyword                 1.000000      0.041089  \n",
      "comment_hour                            0.041089      1.000000  \n",
      "\n",
      "[22 rows x 22 columns]\n",
      "2025-08-14 16:18:46,786 - INFO - eda.stats - Table data saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\pearson_correlation\\RQ5_pearson_correlation_matrix_data.txt\n",
      "2025-08-14 16:18:48,054 - INFO - eda.stats - Plot saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\figures\\pearson_correlation\\RQ5_pearson_correlation_matrix.png\n",
      "2025-08-14 16:18:48,055 - INFO - eda.stats - \n",
      "--- Generating Additional Specific Plots ---\n",
      "2025-08-14 16:18:48,055 - INFO - eda.stats - \n",
      "--- All EDA Plots generated. ---\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ### Reddit Scraper for DotA2 Subreddit\n",
    "#\n",
    "# **Guideline:** Online Engagement with Organizational News on Reddit: An Analysis Using the Example of E-Sports Player Transfers and the Online Presence of Organizations in Rankings and Tournaments.\n",
    "#\n",
    "# ---\n",
    "#\n",
    "# #### 1. Project Setup and Data Preparation\n",
    "#\n",
    "# This section handles the initial setup, including importing necessary libraries, and orchestrating the data collection, cleaning, and feature engineering process.\n",
    "\n",
    "# %% [markdown]\n",
    "# ##### 1.1 Imports and Environment Setup\n",
    "#\n",
    "# Importing all required libraries and configuring the Python environment for modularized functions.\n",
    "\n",
    "# %% [python]\n",
    "import pandas as pd\n",
    "import sys # Required for sys.path manipulation\n",
    "import os  # Required for os.path operations\n",
    "import praw # Required for Reddit API interaction\n",
    "import logging # Required for logging configuration\n",
    "\n",
    "# --- START: Project Root Setup for Module Imports ---\n",
    "# Dynamically determine the project root relative to the notebook's location.\n",
    "# This assumes the notebook is located at:\n",
    "# project_root/BA/notebooks/1_data_extraction/your_notebook.ipynb\n",
    "# So, we need to go up three directories from the current working directory.\n",
    "current_notebook_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(current_notebook_dir, '..', '..', '..'))\n",
    "\n",
    "# Add the project_root to sys.path if it's not already there.\n",
    "# This must happen *before* importing any modules from the project root (like 'config').\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "# --- END: Project Root Setup for Module Imports ---\n",
    "\n",
    "# Import the centralized configuration\n",
    "import config\n",
    "\n",
    "# --- START: Centralized Logging Configuration for Notebook ---\n",
    "# Ensure the log directory exists before configuring logging\n",
    "os.makedirs(config.LOGS_DIR, exist_ok=True)\n",
    "\n",
    "# Logger for data pipeline (prepare_data)\n",
    "data_logger = setup_logger(\n",
    "    \"data.pipeline\",\n",
    "    config.LOG_FILES[\"data.pipeline\"],\n",
    "    config.LOG_LEVEL\n",
    ")\n",
    "\n",
    "# Logger for EDA/ statistical tests (plots; statistical_tests)\n",
    "eda_stats_logger = setup_logger(\n",
    "    \"eda.stats\",\n",
    "    config.LOG_FILES[\"eda.stats\"],\n",
    "    config.LOG_LEVEL\n",
    ")\n",
    "\n",
    "# --- END: Centralized Logging Configuration for Notebook ---\n",
    "\n",
    "\n",
    "# Import custom modules from src/\n",
    "from BA.src.data.database_utils import load_data_from_sqlite  # Handles loading data from the sqlite database\n",
    "from BA.src.visualization.plots import generate_all_eda_plots, plot_pearson_correlation # Orchestrates all EDA plots, and new correlation plot\n",
    "from BA.src.data.prepare_data import prepare_data # Orchestrates data collection and preprocessing\n",
    "from BA.src.analysis.statistical_tests import perform_independent_t_test, perform_anova_test, perform_chi_squared_test # Performs statistical tests\n",
    "from BA.src.utils.logging_utils import setup_logger  # Sets up logging for the project\n",
    "\n",
    "# For plotting (general setup)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\") # Set plot style\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ##### 1.2 Data Collection, Cleaning, and Feature Engineering\n",
    "#\n",
    "# This section orchestrates the entire data preparation pipeline by calling the modularized `prepare_data` function.\n",
    "# It conditionally loads from SQLite or (if not present) collects data from Reddit, cleans, engineers features,\n",
    "# and stores the processed dataset. All parameters are read from `config.py`.\n",
    "\n",
    "# **Note on Data Collection:**\n",
    "#\n",
    "# The `prepare_data()` function is designed to collect and process data from the Reddit API only if it's not already present in the SQLite database (`reddit_dota2_analysis.db`).\n",
    "# This saves time and respects API limits.\n",
    "#\n",
    "# IMPORTANT: To apply changes to data processing (e.g., new feature engineering),\n",
    "# delete the existing database file before re-running the pipeline.\n",
    "#\n",
    "# Windows:     del BA\\data\\processed\\reddit_dota2_analysis.db\n",
    "# macOS/Linux: rm BA/data/processed/reddit_dota2_analysis.db\n",
    "#\n",
    "# After deleting the database, `prepare_data()` will automatically collect the data again on its next call.\n",
    "#\n",
    "# If you wish to manually trigger just the full data extraction, preparation, cleaning and feature engineering process, please run the \"prepare_data.py\" script directly in your terminal of this notebook's environment:\n",
    "#\n",
    "# Windows:     python .\\BA\\src\\data\\prepare_data.py\n",
    "# macOS/Linux: python BA/src/data/prepare_data.py\n",
    "\n",
    "\n",
    "# %% [python]\n",
    "data_logger.info(\"Starting data preparation and feature engineering pipeline...\")\n",
    "\n",
    "# Call the prepare_data function to execute the full pipeline.\n",
    "# It will return the combined and cleaned DataFrame.\n",
    "# No arguments are passed here, as prepare_data now reads them from config.py\n",
    "df_combined_cleaned = prepare_data()\n",
    "\n",
    "data_logger.info(\"Data preparation and feature engineering pipeline complete.\")\n",
    "\n",
    "# Confirmation of successful data preparation\n",
    "if not df_combined_cleaned.empty:\n",
    "    data_logger.info(f\"Combined DataFrame successfully created with shape: {df_combined_cleaned.shape}\")\n",
    "    data_logger.info(\"Combined Cleaned DataFrame Info:\")\n",
    "    df_combined_cleaned.info() # This prints to stdout, not necessarily to logger\n",
    "    print(\"\\nDescriptive statistics of the combined DataFrame:\")\n",
    "    print(df_combined_cleaned.describe(include='all')) # Display descriptive statistics\n",
    "else:\n",
    "    data_logger.warning(\"Combined DataFrame is empty after data preparation.\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ---\n",
    "#\n",
    "# #### 2. Data Loading and Initial Analysis\n",
    "#\n",
    "# This section focuses on loading the processed data from the database and performing initial analytical steps.\n",
    "\n",
    "# %% [markdown]\n",
    "# ##### 2.1 Data Loading from SQLite Database\n",
    "#\n",
    "# Loading the cleaned and feature-engineered data from the previous step by renaming it to df_comments, instead of loading a separate database file with the same data.\n",
    "\n",
    "# %% [python]\n",
    "data_logger.info(\"\\n--- Loading data from SQLite database ---\")\n",
    "# The df_combined_cleaned DataFrame from the previous step is already loaded and processed.\n",
    "df_comments = df_combined_cleaned\n",
    "\n",
    "if not df_comments.empty:\n",
    "    data_logger.info(f\"Comments DataFrame loaded with shape: {df_comments.shape}\")\n",
    "    data_logger.info(\"\\nComments DataFrame Info:\")\n",
    "    df_comments.info() # This prints to stdout, not necessarily to logger\n",
    "    print(\"\\nDescriptive statistics of the loaded DataFrame:\")\n",
    "    print(df_comments.describe(include='all')) # Display descriptive statistics\n",
    "else:\n",
    "    data_logger.warning(\"No data loaded. Please check previous steps.\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ##### 2.2 Pearson Correlations\n",
    "#\n",
    "# Calculating Pearson correlations between selected numerical features to identify potential relationships.\n",
    "# This analysis is now encapsulated in a dedicated function within `BA/src/visualization/plots.py`.\n",
    "\n",
    "# %% [markdown]\n",
    "# #### Interpretation of Pearson Correlation Matrix\n",
    "#\n",
    "# The Pearson Correlation Matrix provides insights into the linear relationships between pairs of numerical variables. The correlation coefficient ranges from -1 (perfect negative linear correlation) to +1 (perfect positive linear correlation), with 0 indicating no linear correlation.\n",
    "#\n",
    "# **Key Observations:**\n",
    "#\n",
    "# *   **Highly Correlated Features (r > 0.9 or r < -0.9):**\n",
    "#     *   `post_title_word_count` and `post_title_length` (r = 0.97): As expected, longer titles tend to have more words.\n",
    "#     *   `word_count` and `char_count` (r = 0.96): Similarly, comments with more characters naturally have more words.\n",
    "#     *   These high correlations suggest that one of these features might be redundant for certain analyses, though tree-based models like XGBoost are generally robust to multicollinearity.\n",
    "#\n",
    "# *   **Strong Positive Correlations (0.7 < r <= 0.9):**\n",
    "#     *   `contains_team_name` and `contains_player_keyword` (r = 0.69): This indicates that posts mentioning team names often also mention player keywords, suggesting discussions around team rosters or player performances.\n",
    "#     *   `compound_sentiment` and `pos_sentiment` (r = 0.67): A higher positive sentiment score strongly contributes to the overall compound sentiment.\n",
    "#\n",
    "# *   **Moderate Positive Correlations (0.3 < r <= 0.7):**\n",
    "#     *   `comment_score` and `post_score` (r = 0.55): Posts with higher scores tend to attract comments that also receive higher scores. This is a significant relationship for understanding engagement.\n",
    "#     *   `is_self` and `post_num_comments` (r = 0.41): Text-based posts (`is_self`) tend to generate more comments, which is an interesting finding regarding content type and interaction.\n",
    "#     *   `contains_event_keyword` and `word_count` (r = 0.37): Comments containing event-related keywords are moderately correlated with longer comments.\n",
    "#\n",
    "# *   **Strong Negative Correlations (-0.7 <= r < -0.3):**\n",
    "#     *   `pos_sentiment` and `neu_sentiment` (r = -0.72): This is a logical inverse relationship; as positive sentiment increases, neutral sentiment tends to decrease, given that the sum of sentiment scores often approaches 1.\n",
    "#     *   `compound_sentiment` and `neg_sentiment` (r = -0.59): Higher negative sentiment naturally leads to a lower overall compound sentiment.\n",
    "#\n",
    "# *   **Weak or Negligible Correlations (r close to 0):**\n",
    "#     *   Many features show very low correlations with `contains_question` and `comment_hour`. This suggests that there isn't a strong linear relationship between these variables and others in the dataset. It's important to remember that a low linear correlation does not rule out non-linear relationships.\n",
    "#\n",
    "# *   **Notable Observations:**\n",
    "#     *   `author_karma` shows `NaN` for all correlations, indicating missing data for this feature. If author karma is deemed important for the research questions, this data gap needs to be addressed.\n",
    "#     *   `upvote_ratio` shows a very weak negative correlation with `comment_score` (r = -0.097). This might be counter-intuitive, as one might expect highly upvoted posts to have highly scored comments. This could suggest that controversial posts (which might have a lower upvote ratio due to mixed reactions) can still generate significant discussion and highly scored comments.\n",
    "#\n",
    "# **Implications:**\n",
    "# The correlation matrix helps identify potential relationships between features, guiding further analysis and feature selection for predictive modeling. Strong correlations can indicate redundant features or important underlying dynamics. Weak correlations suggest that variables might be independent or have non-linear relationships. It is crucial to remember that correlation does not imply causation.\n",
    "\n",
    "# %% [markdown]\n",
    "# ##### 2.3 Statistical Tests\n",
    "#\n",
    "# Performing statistical tests to quantitatively assess differences and associations between variables.\n",
    "# These tests provide the statistical foundation for addressing the research questions by evaluating\n",
    "# whether observed differences in engagement metrics or sentiment across groups are statistically significant.\n",
    "\n",
    "# %% [python]\n",
    "if not df_comments.empty:\n",
    "    eda_stats_logger.info(\"\\n--- Performing Statistical Tests ---\")\n",
    "\n",
    "    # --- Tests for Research Question: How do different factors influence user interactions? ---\n",
    "    # Independent Samples t-tests are applied to compare the mean compound sentiment between\n",
    "    # two selected events, checking if differences in sentiment are statistically significant.\n",
    "\n",
    "    # Independent Samples t-test for Compound Sentiment between TI8 and TI11\n",
    "    # This test evaluates whether the average compound sentiment of comments differs significantly\n",
    "    # between The International 2018 (TI8) and The International 2022 (TI11).\n",
    "    print(\"\\n--- Independent Samples t-test for Compound Sentiment (TI8 vs. TI11) ---\")\n",
    "    perform_independent_t_test(df_comments, 'event_name', 'compound_sentiment', 'TI8', 'TI11')\n",
    "\n",
    "    # %% [markdown]\n",
    "    # #### Interpretation of Independent Samples t-test (TI8 vs. TI11 for Compound Sentiment)\n",
    "    # Interpretation: There is no statistically significant difference (p=0.108) in 'compound_sentiment'\n",
    "    # between 'TI8' (Mean: 0.24) and 'TI11' (Mean: 0.19).\n",
    "\n",
    "    # %% [python]\n",
    "    # Independent Samples t-test for Compound Sentiment between TI8 and OG_RM24\n",
    "    # This test investigates whether the compound sentiment differs significantly between TI8 and OG’s participation\n",
    "    # in the Riyadh Masters 2024 (OG_RM24).\n",
    "    print(\"\\n--- Independent Samples t-test for Compound Sentiment (TI8 vs. OG_RM24) ---\")\n",
    "    perform_independent_t_test(df_comments, 'event_name', 'compound_sentiment', 'TI8', 'OG_RM24')\n",
    "\n",
    "    # %% [markdown]\n",
    "    # #### Interpretation of Independent Samples t-test (TI8 vs. OG_RM24 for Compound Sentiment)\n",
    "    # Interpretation: There is a statistically significant difference (p=0.003) in 'compound_sentiment'\n",
    "    # between 'TI8' (Mean: 0.24) and 'OG_RM24' (Mean: 0.16).\n",
    "\n",
    "    # %% [python]\n",
    "    # Independent Samples t-test for Compound Sentiment between TI11 and OG_RM24\n",
    "    # This test compares TI11 and OG_RM24 to identify if sentiment differs between these events.\n",
    "    print(\"\\n--- Independent Samples t-test for Compound Sentiment (TI11 vs. OG_RM24) ---\")\n",
    "    perform_independent_t_test(df_comments, 'event_name', 'compound_sentiment', 'TI11', 'OG_RM24')\n",
    "\n",
    "    # %% [markdown]\n",
    "    # #### Interpretation of Independent Samples t-test (TI11 vs. OG_RM24 for Compound Sentiment)\n",
    "    # Interpretation: There is no statistically significant difference (p=0.205) in 'compound_sentiment'\n",
    "    # between 'TI11' (Mean: 0.19) and 'OG_RM24' (Mean: 0.16).\n",
    "\n",
    "    # %% [python]\n",
    "    # Independent Samples t-test for Compound Sentiment between OG_RM24 and Topson_RM24\n",
    "    # This test examines whether sentiment differs significantly between comments mentioning OG as a team\n",
    "    # and those mentioning player Topson during Riyadh Masters 2024.\n",
    "    print(\"\\n--- Independent Samples t-test for Compound Sentiment (OG_RM24 vs. Topson_RM24) ---\")\n",
    "    perform_independent_t_test(df_comments, 'event_name', 'compound_sentiment', 'OG_RM24', 'Topson_RM24')\n",
    "\n",
    "    # %% [markdown]\n",
    "    # #### Interpretation of Independent Samples t-test (OG_RM24 vs. Topson_RM24 for Compound Sentiment)\n",
    "    # Interpretation: There is no statistically significant difference (p=0.964) in 'compound_sentiment'\n",
    "    # between 'OG_RM24' (Mean: 0.16) and 'Topson_RM24' (Mean: 0.16).\n",
    "\n",
    "    # %% [markdown]\n",
    "    # ##### One-Way ANOVA Tests\n",
    "    # One-Way ANOVA tests are used to determine whether there are statistically significant differences in mean values\n",
    "    # of a metric across more than two groups.\n",
    "\n",
    "    # %% [python]\n",
    "    # One-Way ANOVA for Comment Score across Time Periods\n",
    "    # This test assesses whether the mean comment score differs significantly between the defined time periods:\n",
    "    # Before, During, After Event and Outside Window relative to an event.\n",
    "    print(\"\\n--- One-Way ANOVA for Comment Score across Time Periods ---\")\n",
    "    perform_anova_test(df_comments, 'time_period', 'comment_score')\n",
    "\n",
    "    # %% [markdown]\n",
    "    # #### Interpretation of One-Way ANOVA (Comment Score across Time Periods)\n",
    "    # Interpretation: There is no statistically significant difference (p=0.519) in 'comment_score'\n",
    "    # across the groups in 'time_period'.\n",
    "\n",
    "    # %% [python]\n",
    "    # One-Way ANOVA for Compound Sentiment across Time Periods\n",
    "    # This test checks if sentiment scores differ significantly between the defined time periods relative to an event.\n",
    "    print(\"\\n--- One-Way ANOVA for Compound Sentiment across Time Periods ---\")\n",
    "    perform_anova_test(df_comments, 'time_period', 'compound_sentiment')\n",
    "\n",
    "    # %% [markdown]\n",
    "    # #### Interpretation of One-Way ANOVA (Compound Sentiment across Time Periods)\n",
    "    # Interpretation: There is no statistically significant difference (p=0.374) in 'compound_sentiment'\n",
    "    # across the groups in 'time_period'.\n",
    "\n",
    "    # %% [python]\n",
    "    # One-Way ANOVA for Comment Score across Post Types\n",
    "    # This test examines whether engagement (measured by comment_score) varies significantly\n",
    "    # between post types such as player transfers, tournament results, and ranking placements.\n",
    "    print(\"\\n--- One-Way ANOVA for Comment Score across Post Types ---\")\n",
    "    perform_anova_test(df_comments, 'post_type', 'comment_score')\n",
    "\n",
    "    # %% [markdown]\n",
    "    # #### Interpretation of One-Way ANOVA (Comment Score across Post Types)\n",
    "    # Interpretation: There is a statistically significant difference (p=0.001) in 'comment_score'\n",
    "    # across at least two groups in 'post_type'.\n",
    "\n",
    "    # %% [python]\n",
    "    # One-Way ANOVA for Compound Sentiment across Post Types\n",
    "    # This test determines if sentiment varies significantly between different post types.\n",
    "    print(\"\\n--- One-Way ANOVA for Compound Sentiment across Post Types ---\")\n",
    "    perform_anova_test(df_comments, 'post_type', 'compound_sentiment')\n",
    "\n",
    "    # %% [markdown]\n",
    "    # #### Interpretation of One-Way ANOVA (Compound Sentiment across Post Types)\n",
    "    # Interpretation: There is a statistically significant difference (p=0.015) in 'compound_sentiment'\n",
    "    # across at least two groups in 'post_type'.\n",
    "\n",
    "    # %% [markdown]\n",
    "    # ##### Chi-squared Tests\n",
    "    # Chi-squared tests of independence are applied to examine whether two categorical variables are associated.\n",
    "\n",
    "    # %% [python]\n",
    "    # Chi-squared test for Time Period and Contains Question\n",
    "    # This test evaluates whether the occurrence of questions in comments is related to the time period\n",
    "    # relative to an event.\n",
    "    print(\"\\n--- Chi-squared test for Time Period and Contains Question ---\")\n",
    "    perform_chi_squared_test(df_comments, 'time_period', 'contains_question')\n",
    "\n",
    "    # %% [markdown]\n",
    "    # #### Interpretation of Chi-squared test (Time Period and Contains Question)\n",
    "    # Interpretation: There is no statistically significant association (p=0.493) between 'time_period' and 'contains_question'.\n",
    "    # This suggests that the two variables are independent.\n",
    "\n",
    "    # %% [python]\n",
    "    # Chi-squared test for Event Name and Contains Team Name\n",
    "    # This test determines whether mentions of team names in comments are associated with specific events.\n",
    "    print(\"\\n--- Chi-squared test for Event Name and Contains Team Name ---\")\n",
    "    perform_chi_squared_test(df_comments, 'event_name', 'contains_team_name')\n",
    "\n",
    "    # %% [markdown]\n",
    "    # #### Interpretation of Chi-squared test (Event Name and Contains Team Name)\n",
    "    # Interpretation: There is a statistically significant association (p=0.000) between 'event_name' and 'contains_team_name'.\n",
    "    # This suggests that the two variables are not independent.\n",
    "\n",
    "    # %% [python]\n",
    "    eda_stats_logger.info(\"\\n--- Statistical Tests Complete ---\")\n",
    "else:\n",
    "    eda_stats_logger.info(\"Comments DataFrame is empty, skipping statistical tests.\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ---\n",
    "#\n",
    "# #### 3. Exploratory Data Analysis (EDA) and Visualization\n",
    "#\n",
    "# This section focuses on visually exploring the cleaned and feature-engineered data to identify patterns, trends, and differences between the four variables. All plots are generated via a single orchestrating function.\n",
    "\n",
    "# %% [markdown]\n",
    "# ##### 3.1 Interpretation of Distribution Plots\n",
    "#\n",
    "# When analyzing the distribution plots (histograms with KDE, and the new Box/Violin plots), focus on the following aspects for a comprehensive understanding:\n",
    "#\n",
    "# *   **KDE Curve (Kernel Density Estimate):** This smoothed line is crucial for understanding the *shape* of the distribution. Look for:\n",
    "#     *   **Peaks (Modes):** Where are the most frequent values concentrated? Are there multiple peaks?\n",
    "#     *   **Skewness:** Is the distribution symmetrical, or is it skewed to the left (tail on the left) or right (tail on the right)?\n",
    "#     *   **Spread:** How wide is the distribution? Does it cover a large range of values or are they tightly clustered?\n",
    "#     *   **Comparison:** When comparing multiple KDE curves (e.g., for different events), observe if their shapes are similar, if their peaks align, and if their spreads are comparable.\n",
    "#\n",
    "# *   **Box Plots / Violin Plots:** These plots are excellent for comparing distributions *between different groups* (e.g., events).\n",
    "#     *   **Box Plots:** Clearly show the median (middle line), the interquartile range (IQR, the box itself, representing the middle 50% of data), and potential outliers (individual points). They are great for quickly assessing central tendency, spread, and skewness.\n",
    "#     *   **Violin Plots:** Combine the box plot with the KDE, showing the density of the data at different values. They provide a richer view of the distribution's shape than box plots, especially useful for identifying multimodal distributions.\n",
    "#\n",
    "# *   **Quantitative Metrics (Mean, Median, Standard Deviation):** These numerical summaries, provided next to the plots, are essential for precise, quantitative comparisons.\n",
    "#     *   **Mean & Median:** Indicate the *central tendency* of the data. Compare these values across groups to see if the \"average\" or \"typical* value differs.\n",
    "#     *   **Standard Deviation:** Measures the *spread* or *variability* of the data. A larger standard deviation means data points are more spread out from the mean.\n",
    "#\n",
    "# **In summary:** While histograms give a raw count, the **KDE curve** helps you understand the *overall shape* of the distribution. The **Box/Violin plots** are superior for *comparing distributions across groups*. The **numerical summary statistics** provide the *precise quantitative details* to support your visual observations and are critical for your analysis.\n",
    "\n",
    "# %% [python]\n",
    "# Generate all EDA plots using the modularized function\n",
    "if not df_comments.empty:\n",
    "    generate_all_eda_plots(df_comments)\n",
    "else:\n",
    "    eda_stats_logger.info(\"Comments DataFrame is empty, skipping EDA plots.\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ---\n",
    "#\n",
    "# #### 4. Machine Learning Preparation\n",
    "#\n",
    "# This section outlines the preparation steps for machine learning, with core functionalities modularized into external scripts, which is found within the `BA/src/models/train_model.py` script.\n",
    "\n",
    "# %% [markdown]\n",
    "# ---\n",
    "#\n",
    "# #### 5. Model Preparation and Data Splitting\n",
    "#\n",
    "# The process of splitting the data into training and testing sets, as well as the final assembly of the feature matrix, is handled within the `BA/src/models/train_model.py` script.\n",
    "\n",
    "# %% [markdown]\n",
    "# ---\n",
    "#\n",
    "# #### 6. Model Training and Evaluation\n",
    "#\n",
    "# The end-to-end ML pipeline (training, hyperparameter tuning, cross-validation, evaluation, SHAP plots, artifact saving)\n",
    "# is orchestrated by `BA/src/models/train_model.py`.\n",
    "# The machine learning pipeline is designed to be executed as a standalone script, ensuring a clean and reproducible workflow.\n",
    "#\n",
    "# To execute the full machine learning pipeline, please run the `train_model.py` script directly from your terminal:\n",
    "#\n",
    "# Windows:     python .\\BA\\src\\models\\train_model.py\n",
    "# Linux/MacOS: python BA/src/models/train_model.py\n",
    "#\n",
    "# This modular approach enhances reproducibility, maintainability, and allows for easier integration into automated workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLDebug",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
